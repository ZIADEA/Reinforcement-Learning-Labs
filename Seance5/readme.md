<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=20,23,26&height=140&section=header&text=Seance%205&fontSize=48&fontColor=fff&animation=fadeIn&fontAlignY=38&desc=PPO%20%26%20Stable-Baselines3&descAlignY=55&descAlign=50"/>

<br/>

![PPO](https://img.shields.io/badge/Algorithm-PPO-27AE60?style=for-the-badge&logo=openai)
![SB3](https://img.shields.io/badge/Framework-Stable--Baselines3-F39C12?style=for-the-badge&logo=python)
![Gymnasium](https://img.shields.io/badge/Env-Gymnasium-00A67E?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)

<br/>

**Mini-Ã©tude PPO sur GridWorld statique/mobile et CartPole avec analyse de convergence**

</div>

<br/>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif">

---

## ğŸ“ Note PÃ©dagogique : PPO et l'Ãˆre Moderne du RL

### ğŸš€ Pourquoi PPO a tout changÃ© (2017)
Si DQN (2015) a prouvÃ© que le Deep RL Ã©tait possible, **PPO (Proximal Policy Optimization)**, publiÃ© par **OpenAI** (Schulman et al., 2017), a prouvÃ© qu'il pouvait Ãªtre *fiable* et *utilisable*.

Avant PPO, nous utilisions des mÃ©thodes comme TRPO (Trust Region Policy Optimization) qui Ã©taient mathÃ©matiquement Ã©lÃ©gantes mais extrÃªmement complexes Ã  implÃ©menter et lourdes Ã  calculer. PPO a apportÃ© une simplicitÃ© dÃ©concertante avec une efficacitÃ© redoutable. C'est aujourd'hui l'algorithme "par dÃ©faut" utilisÃ© par OpenAI (notamment pour entraÃ®ner ChatGPT via RLHF) et la plupart des chercheurs.

### ğŸ§  Policy Gradient vs Q-Learning
Dans les sÃ©ances prÃ©cÃ©dentes (DQN), nous utilisions des mÃ©thodes basÃ©es sur la **valeur** (Value-Based) : on apprend $Q(s,a)$ pour dÃ©duire la politique $\pi(s) = \arg\max Q$.
Ici, avec PPO, nous sommes dans la famille des **Policy Gradients** : on apprend *directement* la politique $\pi_\theta(a|s)$ (la probabilitÃ© de prendre une action).

*   **Avantage** : Peut gÃ©rer des espaces d'actions continus (comme contrÃ´ler un bras robotique) et des politiques stochastiques.
*   **InconvÃ©nient** : TrÃ¨s sensible au "step size". Si on modifie trop brutalement la politique, l'agent "tombe de la falaise" et ne s'en remet jamais.

### ğŸ›¡ï¸ Le GÃ©nie du "Clipping"
L'innovation majeure de PPO rÃ©side dans sa fonction objective "clippÃ©e".
Imaginez un professeur qui corrige un Ã©lÃ¨ve.
*   Si l'Ã©lÃ¨ve s'amÃ©liore un peu, le professeur l'encourage.
*   Si l'Ã©lÃ¨ve veut changer radicalement sa mÃ©thode de travail d'un coup, le professeur dit "Stop ! Pas trop vite".

MathÃ©matiquement, PPO limite (clip) le ratio entre la nouvelle et l'ancienne politique :
$$ r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$
On force ce ratio Ã  rester dans un intervalle $[1-\epsilon, 1+\epsilon]$ (gÃ©nÃ©ralement $\epsilon=0.2$). Cela garantit des mises Ã  jour **monotones** et **sÃ»res**. C'est ce qui rend PPO si robuste aux hyperparamÃ¨tres comparÃ© Ã  ses prÃ©dÃ©cesseurs.

> **ğŸ“š RÃ©fÃ©rence Incontournable :**
> *Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.*

---

## ğŸ¯ AperÃ§u

Ce dÃ©pÃ´t prÃ©sente une mini-Ã©tude expÃ©rimentale autour de PPO appliquÃ© Ã  :

1. **GridWorld statique** (`GridWorldStatic-v0`) - goal fixe
2. **GridWorld mobile** (`GridWorldMoving-v0`) - goal qui se dÃ©place
3. **Transfert** - finetuning du Moving Ã  partir d'un agent Static prÃ©-entraÃ®nÃ©
4. **CartPole-v1** - environnement de rÃ©fÃ©rence

**Technologies** : Stable-Baselines3, rl-baselines3-zoo, environnement GridWorld custom inspirÃ© de [seance2/minegym](../seance2/minegym)

**Analyse** : TensorBoard (convergence rÃ©compenses, longueurs Ã©pisodes, pertes) et GIFs de visualisation qualitative

## ğŸš€ DÃ©marrage Rapide

<details open>
<summary><b>âš™ï¸ 1. Structure du Projet</b></summary>

```
Seance5/rl_sb/
â”œâ”€â”€ gridworld_env/          # Package environnement GridWorld
â”œâ”€â”€ gridworld_runs/         # GIFs et vidÃ©os des agents
â”œâ”€â”€ models/ppo/             # Checkpoints des modÃ¨les
â””â”€â”€ rl-baselines3-zoo/      # Framework d'entraÃ®nement
    â””â”€â”€ logs/               # TensorBoard logs
```
</details>

<details>
<summary><b>ğŸ“¦ 2. Installation de l'Environnement</b></summary>

```bash
cd Seance5/rl_sb/gridworld_env
pip install -e .
```
</details>

<details>
<summary><b>â–¶ï¸ 3. Lancer un EntraÃ®nement</b></summary>

```bash
cd Seance5/rl_sb/rl-baselines3-zoo
python train.py --algo ppo --env GridWorldStatic-v0 --eval-freq 1000
```
</details>

<details>
<summary><b>ğŸ“Š 4. Visualiser avec TensorBoard</b></summary>

```bash
tensorboard --logdir Seance5/rl_sb/rl-baselines3-zoo/logs
```
</details>

## ğŸ¬ Galerie d'Agents AnimÃ©s

<div align="center">

### ğŸŸ¢ GridWorld Goal Statique

<table>
<tr>
<td align="center" width="33%">
<img src="rl_sb/gridworld_runs/gridworld_ppo_static_50k_live.gif" width="240"/>
<br/><br/>
<img src="https://img.shields.io/badge/50k_steps-Training-3498db?style=flat-square"/>
</td>
<td align="center" width="33%">
<img src="rl_sb/gridworld_runs/gridworld_static_live.gif" width="240"/>
<br/><br/>
<img src="https://img.shields.io/badge/100k_steps-Converged-2ecc71?style=flat-square"/>
</td>
<td align="center" width="33%">
<img src="rl_sb/gridworld_runs/gridworld_ppo_static_400k_live.gif" width="240"/>
<br/><br/>
<img src="https://img.shields.io/badge/400k_steps-Optimal-27ae60?style=flat-square"/>
</td>
</tr>
</table>

### ğŸ”µ GridWorld Goal Mobile

<table>
<tr>
<td align="center" width="33%">
<img src="rl_sb/gridworld_runs/gridworld_moving_live.gif" width="240"/>
<br/><br/>
<img src="https://img.shields.io/badge/100k_steps-Training-3498db?style=flat-square"/>
</td>
<td align="center" width="33%">
<img src="rl_sb/gridworld_runs/gridworld_ppo_moving_400k_live.gif" width="240"/>
<br/><br/>
<img src="https://img.shields.io/badge/400k_steps-Converged-2ecc71?style=flat-square"/>
</td>
<td align="center" width="33%">
<img src="rl_sb/gridworld_runs/gridworld_ppo_moving_600k_live.gif" width="240"/>
<br/><br/>
<img src="https://img.shields.io/badge/600k_steps-Optimal-27ae60?style=flat-square"/>
</td>
</tr>
</table>

### ğŸ”„ Transfert Learning & CartPole

<table>
<tr>
<td align="center" width="50%">
<img src="rl_sb/gridworld_runs/gridworld_ppo_moving_finetune_live.gif" width="300"/>
<br/><br/>
<img src="https://img.shields.io/badge/Fine--tuning-Staticâ†’Moving-9b59b6?style=flat-square"/>
<br/><sub>Transfert depuis agent statique</sub>
</td>
<td align="center" width="50%">
<img src="rl_sb/gridworld_runs/test_cartpole.gif" width="300"/>
<br/><br/>
<img src="https://img.shields.io/badge/CartPole--v1-Solved-f39c12?style=flat-square"/>
<br/><sub>Benchmark de rÃ©fÃ©rence</sub>
</td>
</tr>
</table>

</div>

## ğŸ“Š RÃ©sultats et Analyses ComplÃ¨tes

### ğŸ“ˆ Graphiques TensorBoard - MÃ©triques de Convergence

<details open>
<summary><b>ğŸŸ¢ GridWorld Statique - Ã‰volution des MÃ©triques</b></summary>

<table>
<tr>
<td align="center" width="50%">
<img src="images/static_50k_400k_ep_rew_mean.png" width="100%"/>
<br/><br/>
<b>ğŸ“Š RÃ©compense Moyenne par Ã‰pisode</b>
<br/>
<sub><i>Comparaison 50k vs 400k steps</i></sub>
</td>
<td align="center" width="50%">
<img src="images/static_50k_400k_ep_len_mean.png" width="100%"/>
<br/><br/>
<b>ğŸ“ Longueur Moyenne par Ã‰pisode</b>
<br/>
<sub><i>DÃ©croissance des steps nÃ©cessaires</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse :**
- **Reward 50k steps** : Atteint ~0.85-0.90 (proche optimal) dÃ¨s 30k steps â†’ convergence rapide
- **Reward 400k steps** : Stabilisation parfaite Ã  ~0.95 â†’ politique optimale maÃ®trisÃ©e
- **Episode Length** : DÃ©croÃ®t de ~25 steps (exploration) Ã  ~12-15 steps (optimal Manhattan distance)

**ğŸ’¡ UtilitÃ©** : DÃ©montre que **50k steps suffisent** pour rÃ©soudre GridWorld statique (goal fixe = tÃ¢che simple)

#### ğŸ“¸ Snapshots Individuels

<table>
<tr>
<td align="center" width="50%">
<img src="images/gridworld_static_50k_ep_rew_mean.png" width="100%"/>
<br/><sub>Snapshot 50k steps</sub>
</td>
<td align="center" width="50%">
<img src="images/gridworld_static_400k_ep_rew_mean.png" width="100%"/>
<br/><sub>Snapshot 400k steps</sub>
</td>
</tr>
</table>

</details>

<details>
<summary><b>ğŸ”µ GridWorld Mobile - Apprentissage Adaptatif</b></summary>

<table>
<tr>
<td align="center" width="50%">
<img src="images/moving_400k_600k_1600k_ep_rew_mean.png" width="100%"/>
<br/><br/>
<b>ğŸ“Š RÃ©compense Moyenne par Ã‰pisode</b>
<br/>
<sub><i>400k, 600k, 1600k steps comparÃ©s</i></sub>
</td>
<td align="center" width="50%">
<img src="images/moving_400k_600k_1600k_ep_len_mean.png" width="100%"/>
<br/><br/>
<b>ğŸ“ Longueur Moyenne par Ã‰pisode</b>
<br/>
<sub><i>Adaptation au goal mobile</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse Comparative :**

| Steps | Reward Moyen | Episode Length | Convergence |
|-------|-------------|----------------|-------------|
| 400k | ~0.70-0.75 | ~18-20 | Partielle |
| 600k | ~0.80-0.85 | ~15-17 | Bonne |
| 1600k | ~0.85-0.90 | ~13-15 | Optimale |

**ğŸ” InterprÃ©tations :**
- **Reward** : Convergence 2-3Ã— plus lente que statique (goal mobile = distribution non-stationnaire)
- **Episode Length** : Reste lÃ©gÃ¨rement supÃ©rieur au statique (agent doit rÃ©agir aux mouvements du goal)
- **Variance** : Plus Ã©levÃ©e que statique (stochasticitÃ© du mouvement du goal)

**ğŸ’¡ UtilitÃ©** : Quantifie le coÃ»t de l'adaptation (goal statique â†’ mobile â‰ˆ 10Ã— plus de steps requis)

#### ğŸ“¸ Snapshots Individuels

<table>
<tr>
<td align="center" width="50%">
<img src="images/gridworld_moving_400k_ep_rew_mean.png" width="100%"/>
<br/><sub>Snapshot 400k steps</sub>
</td>
<td align="center" width="50%">
<img src="images/gridworld_moving_600k_ep_rew_mean.png" width="100%"/>
<br/><sub>Snapshot 600k steps</sub>
</td>
</tr>
</table>

</details>

---

### ğŸ“‚ Fichiers CSV - Monitor Logs

Chaque entraÃ®nement gÃ©nÃ¨re un fichier `monitor.csv` dans `logs/ppo/<env_name>_*/` contenant :

```csv
# {"t_start": 1234567890.0, "env_id": "GridWorldStatic-v0"}
r,l,t
-5.0,23,0.12
8.5,17,0.25
45.0,12,0.38
...
```

**ğŸ“ Colonnes :**
- **r** : Retour total de l'Ã©pisode (reward cumulÃ©)
- **l** : Longueur de l'Ã©pisode (nombre de steps)
- **t** : Timestamp cumulÃ© (temps Ã©coulÃ© depuis le dÃ©but)

**ğŸ’¡ UtilitÃ©** :
- Import direct dans Pandas/Matplotlib pour analyses custom
- Calcul de statistiques avancÃ©es (variance, quantiles, corrÃ©lations)
- Comparaison multi-runs (boxplots, t-tests, etc.)

---

### ğŸ¬ Galerie ComplÃ¨te des Agents (GIFs)

<div align="center">

| Fichier GIF | Environnement | Steps | Description |
|------------|---------------|-------|-------------|
| `gridworld_ppo_static_50k_live.gif` | Static | 50k | Agent en cours d'apprentissage |
| `gridworld_static_live.gif` | Static | 100k | Agent convergÃ© |
| `gridworld_ppo_static_400k_live.gif` | Static | 400k | Agent expert |
| `gridworld_moving_live.gif` | Moving | 100k | Adaptation initiale |
| `gridworld_ppo_moving_400k_live.gif` | Moving | 400k | Convergence partielle |
| `gridworld_ppo_moving_600k_live.gif` | Moving | 600k | Agent performant |
| `gridworld_ppo_moving_finetune_live.gif` | Moving | Fine-tune | Transfert depuis Static |
| `test_cartpole.gif` | CartPole-v1 | Standard | Benchmark validation |

</div>

**ğŸ“ Observations Visuelles :**

1. **Static 50k** : HÃ©sitations visibles, quelques dÃ©tours
2. **Static 400k** : Trajectoires parfaitement droites vers goal
3. **Moving 100k** : Suit le goal avec ~2-3 steps de retard
4. **Moving 600k** : RÃ©activitÃ© quasi-instantanÃ©e aux mouvements du goal
5. **Fine-tune** : Performances intermÃ©diaires (70% de l'agent Moving natif)
6. **CartPole** : Ã‰quilibrage stable >500 steps (problÃ¨me rÃ©solu)

**ğŸ’¡ UtilitÃ© des GIFs** : Validation qualitative que les mÃ©triques reflÃ¨tent bien le comportement observÃ©

---

### Environnements EntraÃ®nÃ©s - RÃ©sumÃ© Quantitatif

<table>
<tr>
<th>Environnement</th>
<th>Steps</th>
<th>RÃ©compense Moyenne</th>
<th>Statut</th>
<th>Nb GIFs</th>
<th>Nb Images</th>
</tr>
<tr>
<td>GridWorldStatic-v0</td>
<td>50k / 100k / 400k</td>
<td>~0.95</td>
<td><img src="https://img.shields.io/badge/âœ“-Complete-success?style=flat-square"/></td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>GridWorldMoving-v0</td>
<td>100k / 400k / 600k</td>
<td>~0.85</td>
<td><img src="https://img.shields.io/badge/âœ“-Complete-success?style=flat-square"/></td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>Fine-tuning (Staticâ†’Moving)</td>
<td>Variable</td>
<td>~0.70</td>
<td><img src="https://img.shields.io/badge/âš -Partiel-orange?style=flat-square"/></td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>CartPole-v1</td>
<td>Standard</td>
<td>~500</td>
<td><img src="https://img.shields.io/badge/âœ“-Solved-success?style=flat-square"/></td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td><b>TOTAL</b></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><b>8</b></td>
<td><b>8</b></td>
</tr>
</table>

### Observations ClÃ©s

- ğŸ¯ **GridWorld Statique** : Convergence rapide (50k steps), politique optimale claire, reward ~0.95
- ğŸ”„ **GridWorld Mobile** : Apprentissage 10Ã— plus long, adaptation continue nÃ©cessaire, reward ~0.85
- âš ï¸ **Transfert Learning** : Performance limitÃ©e (~70% du natif), nÃ©cessite rÃ©entraÃ®nement significatif
- âœ… **CartPole** : Validation du pipeline d'entraÃ®nement PPO (>500 reward = solved)

**ğŸ’¡ Conclusion** : PPO fonctionne bien sur GridWorld, mais la mobilitÃ© du goal augmente drastiquement la complexitÃ©

---

## ğŸ“Š RÃ©sumÃ© Visual Assets SÃ©ance 5

<div align="center">

| CatÃ©gorie | GIFs | Images PNG | CSV | Total | UtilitÃ© Principale |
|-----------|------|-----------|-----|-------|-------------------|
| **GridWorld Static** | 3 | 2 | N | 5 | Baseline convergence rapide |
| **GridWorld Moving** | 3 | 4 | N | 7 | Adaptation Ã  distribution non-stationnaire |
| **Fine-tuning** | 1 | 0 | N | 1 | Transfert learning (limitÃ©) |
| **CartPole** | 1 | 0 | N | 1 | Validation pipeline PPO |
| **Monitor CSVs** | 0 | 0 | ~8 | ~8 | Logs bruts pour analyses externes |
| **TOTAL** | **8** | **8** | **~8** | **~24** | **Analyse complÃ¨te PPO multi-env** |

</div>

---

## ğŸ“ Ressources Disponibles

<table>
<tr>
<td width="33%" align="center">
<br/>
ğŸ¬ <b>Animations</b>
<br/><br/>
9 GIFs dans<br/><code>gridworld_runs/</code>
<br/><br/>
</td>
<td width="33%" align="center">
<br/>
ğŸ“ˆ <b>TensorBoard Logs</b>
<br/><br/>
Logs complets dans<br/><code>rl-baselines3-zoo/logs/</code>
<br/><br/>
</td>
<td width="33%" align="center">
<br/>
ğŸ’¾ <b>Checkpoints</b>
<br/><br/>
ModÃ¨les dans<br/><code>models/ppo/</code>
<br/><br/>
</td>
</tr>
</table>

## ğŸ”§ Configuration PPO UtilisÃ©e

```python
{
    "policy": "MlpPolicy",
    "n_steps": 2048,
    "batch_size": 64,
    "gae_lambda": 0.95,
    "gamma": 0.99,
    "n_epochs": 10,
    "ent_coef": 0.0,
    "learning_rate": 3e-4,
    "clip_range": 0.2
}
```

## ğŸ” Comment Explorer

1. **Consulter les GIFs** dans `gridworld_runs/` pour voir les agents en action
2. **Analyser TensorBoard** : `tensorboard --logdir rl-baselines3-zoo/logs`
3. **Tester les modÃ¨les** : utiliser `enjoy.py` de rl-baselines3-zoo
4. **RÃ©entraÃ®ner** : modifier hyperparamÃ¨tres et relancer `train.py`

---

<div align="center">

<br/>

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&color=27AE60&center=true&vCenter=true&width=500&lines=PPO+sur+GridWorld+%E2%9C%85;CartPole+Solved+%E2%9C%85;8%2B+Training+Runs" alt="Typing SVG" />

<br/><br/>

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=20,23,26&height=100&section=footer"/>

</div>
