<div align="center">

# ğŸ¤– ExpÃ©rience DQN Flexible
## Comparaison NaÃ¯f vs DQN Complet

![DQN](https://img.shields.io/badge/Deep_Q--Network-Complete-red?style=for-the-badge)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0-EE4C2C?style=for-the-badge)
![Linear](https://img.shields.io/badge/Baseline-Linear-blue?style=for-the-badge)

</div>

---
Voici un **README.md** complet, rÃ©digÃ© â€œcomme moiâ€, avec tous les dÃ©tails pratiques, mes choix imposÃ©s (rÃ©compenses, protocole de comparaison), et les explications techniques de lâ€™agent.

---

## ğŸ¯ RÃ©sumÃ©

<table>
<tr>
<td width="50%">

### ğŸ”¬ **Ã‰tude de Comparaison**
- ğŸŸ¦ **NAÃF (LinÃ©aire)** : Pas de replay, pas de rÃ©seau cible
- ğŸŸ¥ **DQN (Complet)** : Replay buffer complet + rÃ©seau cible
- âš–ï¸ **Protocole Ã©quitable** : MÃªmes Ã©pisodes, mÃªmes rÃ©compenses, mÃªme seed

</td>
<td width="50%">

### ğŸ’¾ **Sorties**
- ğŸ“ `figures/flex_<mode>_<lin|mlp>/`
- ğŸ“Š Courbes de perte, graphiques de rÃ©compense, tableaux de bord
- ğŸ¯ `checkpoints/flex_<mode>/`

</td>
</tr>
</table>

> **ğŸ’¡ RÃ©fÃ©rence Rapide :** Section 4 pour les commandes de lancement | Section 8 dÃ©crit toutes les sorties

## ğŸš€ RÃ©sumÃ© DÃ©marrage Rapide

```mermaid
graph LR
    A[ğŸ“¦ Installer deps] --> B[â–¶ï¸ Lancer NAÃF]
    A --> C[â–¶ï¸ Lancer DQN]
    B --> D[ğŸ“ˆ Analyser rÃ©sultats]
    C --> D
```

1. **ğŸ“¦ Installer** Python 3.9+ + `numpy`, `matplotlib`, `torch`, `pillow`, `imageio` (section 2)
2. **â–¶ï¸ Lancer** le mode naÃ¯f ou DQN via les commandes de la section 4
3. **ğŸ“ˆ Analyser** les figures CSV et checkpoints dÃ©crits dans la section 8

---

# README â€” ExpÃ©rience flexible (NAÃF â†” DQN) sur GridWorld
Ce projet me permet de comparer proprement **deux variantes** sur un mÃªme code :

* **NAÃF** : pas de replay buffer, pas de target network, une seule tÃªte *online* mise Ã  jour Ã  chaque transition.
* **DQN complet** : replay buffer + miniâ€batch + rÃ©seau *target* synchronisÃ© pÃ©riodiquement.

Lâ€™environnement est un **GridWorld** (6Ã—6 par dÃ©faut) avec **1 goal tirÃ© Ã  chaque Ã©pisode** (fixe pendant lâ€™Ã©pisode).
Je logge les mÃ©triques dans un CSV, jâ€™enregistre des **plots** standardisÃ©s (loss, reward, steps, Îµ & â€–Î¸â€–, etc.), et je sauvegarde des checkpoints.

---

## 1) Arborescence minimale

```
minegym/
â”œâ”€ envs/
â”‚  â””â”€ gridworld.py                 # GridEnv (fourni)
â”œâ”€ agents/
â”‚  â””â”€ agentDQN_flexible.py         # Agent unique (paramÃ©trable NAÃF <-> DQN)
â”œâ”€ utils/
â”‚  â”œâ”€ plotting.py                  # Fonctions de tracÃ© (dashboard, courbesâ€¦)
â”‚  â””â”€ video.py                     # Outils vidÃ©o (optionnels)
â””â”€ experiments/
   â””â”€ exp_flexible.py              # Script dâ€™expÃ©rience (CLI)
```

---

## 2) Installation rapide

PrÃ©requis :

* Python 3.9+
* `numpy`, `matplotlib`, `torch`, `pillow` (et `imageio` si je veux la vidÃ©o)

```bash
pip install numpy matplotlib torch pillow imageio
```

---

## 3) RÃ©compenses (imposÃ©es)

Je **fixe** les rÃ©compenses suivantes (je ne justifie pas ici) :

```python
reward_step = -3
reward_obstacle_attempt = -10
reward_wall_bump = -5
reward_goal = +35
```

Dans mon code dâ€™expÃ©rience, je construis lâ€™env comme ci-dessous :

```python
env = GridEnv(
    rows=6, cols=6, obstacles=None, goals=None, start=None, seed=0, moving_goal=False,
    reward_step=-3, reward_obstacle_attempt=-10, reward_wall_bump=-5, reward_goal=35
)
```

Si besoin, jâ€™adapte ces 4 valeurs au mÃªme endroit.

---

## 4) Commandes de lancement (comparaison Ã©quitable)

Je garde **le mÃªme budget** pour comparer NAÃF vs DQN : mÃªme `--episodes`, mÃªme `--max-steps`, mÃªme `seed`, mÃªmes rÃ©compenses.

### a) NAÃF (linÃ©aire = pas de hidden layer)

```bash
python -m minegym.experiments.exp_flexible \
  --mode naive --hidden "" --episodes 800 --max-steps 120 --seed 0 \
  --lr 0.05 --loss huber --huber-beta 1.0
```

* `--hidden ""` â‡’ **linÃ©aire** (une couche `Linear(n_statesâ†’n_actions)`).
* LR plus haut en NAÃF + linÃ©aire (0.05) : câ€™est ce que jâ€™utilise.

### b) DQN complet (MLP 64-64)

```bash
python -m minegym.experiments.exp_flexible \
  --mode dqn --hidden "64,64" --episodes 800 --max-steps 120 --seed 0 \
  --lr 1e-3 --loss huber --huber-beta 1.0
```

* DQN active **replay** + **target** automatiquement.
* LR typique 1e-3 avec Adam sur un petit MLP.

> Remarque : Matplotlib est en backend **Agg** â†’ pas dâ€™interface live pendant lâ€™entraÃ®nement, les figures sont directement sauvegardÃ©es.

---

## 5) ParamÃ¨tres CLI (ceux que je rÃ¨gle le plus)

| ParamÃ¨tre               |       DÃ©faut | RÃ´le                                                     |
| ----------------------- | -----------: | -------------------------------------------------------- |
| `--mode`                |      `naive` | `naive` = sans replay/target ; `dqn` = replay+target     |
| `--episodes`            |        `800` | Ã‰pisodes dâ€™entraÃ®nement                                  |
| `--max-steps`           |        `120` | Pas max par Ã©pisode                                      |
| `--seed`                |          `0` | Graine globale                                           |
| `--rows`, `--cols`      |     `6`, `6` | Taille de la grille                                      |
| `--hidden`              |         `""` | `""` â†’ linÃ©aire ; `"64,64"` â†’ MLP                        |
| `--lr`                  |       `1e-3` | Taux dâ€™apprentissage (NAÃF linÃ©aire : je monte Ã  `0.05`) |
| `--gamma`               |       `0.98` | Facteur dâ€™actualisation                                  |
| `--loss`                |        `mse` | `mse` ou `huber`                                         |
| `--huber-beta`          |        `1.0` | Seuil de la Huber                                        |
| `--eps-start/end`       | `1.0 / 0.05` | Epsilon-greedy (dÃ©part/min)                              |
| `--eps-decay-steps`     |       `8000` | DÃ©croissance linÃ©aire de Îµ (steps)                       |
| `--buffer`              |      `50000` | CapacitÃ© replay (DQN)                                    |
| `--batch`               |         `64` | Batch size (DQN)                                         |
| `--warmup`              |       `1000` | Min samples avant train (DQN)                            |
| `--train-every`         |          `1` | FrÃ©quence dâ€™update en steps (DQN)                        |
| `--target-update-steps` |       `5000` | Sync du target (steps)                                   |
| `--device`              |        `cpu` | `cpu` ou `cuda`                                          |
| `--save-dir`            |       `None` | Dossier figures/CSV (auto si None)                       |
| `--eval-every`          |         `50` | Ã‰valuations greedy pÃ©riodiques                           |
| `--eval-episodes`       |         `20` | Nb dâ€™Ã©pisodes pour lâ€™Ã©val                                |

**Internes (agent)** : `gradient_clip=10.0`, optimiseur **Adam**, perte **MSE** ou **Huber**.

---

## 6) Ce que fait le code (rÃ©sumÃ© technique)

### ReprÃ©sentation dâ€™Ã©tat

* Jâ€™encode lâ€™Ã©tat `s` en **one-hot** de dimension `n_states = rows Ã— cols`.
  Exemple 6Ã—6 â†’ 36 features.

### RÃ©seaux

* **Sortie** : 4 scalaires (Q(s,\cdot)) (â† â†’ â†‘ â†“).
* **LinÃ©raire** (`--hidden ""`) : `Linear(36 â†’ 4)` sur une 6Ã—6.
  Câ€™est lâ€™Ã©quivalent dâ€™une **table Q** paramÃ©trique (144 poids + 4 biais).
* **MLP** (`--hidden "64,64"` par ex.) : `Linear(n_statesâ†’64)+ReLU+Linear(64â†’64)+ReLU+Linear(64â†’4)`.
  La **derniÃ¨re couche reste linÃ©aire** (les Q-valeurs ne sont pas bornÃ©es).

### Politique / exploration

* **Îµ-greedy** : alÃ©atoire avec proba Îµ, sinon `argmax_a Q(s,a)`.
* Îµ dÃ©croÃ®t linÃ©airement de `eps_start` Ã  `eps_end` sur `eps_decay_steps` **steps**.

### Cible TD & perte

* **NAÃF** : (Y = r + \gamma \max_{a'} Q_{\text{online}}(s',a')) (ou (r) si `done`).
* **DQN** : (Y = r + \gamma \max_{a'} Q_{\text{target}}(s',a')) (ou (r) si `done`).
* **Perte** : MSE (par dÃ©faut) ou **Huber** (`--loss huber --huber-beta 1.0`).
* **Clipping** de gradient L2 Ã  10.0.

### Replay buffer (DQN)

* MÃ©moire circulaire `(s,a,r,s',done)` de capacitÃ© `--buffer`.
* EntraÃ®nement aprÃ¨s `--warmup` transitions, puis tous les `--train-every` steps avec des mini-batches `--batch`.

### Target network (DQN)

* Copie du *online*, figÃ©e entre deux synchronisations.
* Sync toutes les `--target-update-steps` (en steps).

---

## 7) Comptage de paramÃ¨tres (affichÃ© au dÃ©marrage)

* LinÃ©aire 6Ã—6 â†’ 4 actions : **36Ã—4 + 4 = 148** paramÃ¨tres.
* Exemple **MLP 64-64** (6Ã—6 â†’ 36 entrÃ©es) :

  * 36â†’64 : `36Ã—64 + 64 = 2368`
  * 64â†’64 : `64Ã—64 + 64 = 4160`
  * 64â†’4  : `64Ã—4  + 4  = 260`
  * **Total â‰ˆ 6788** paramÃ¨tres.

Le script affiche `ParamÃ¨tres: N` pour le *online* (et le *target* si DQN).

---

## 8) Fichiers gÃ©nÃ©rÃ©s

Tout est dans `--save-dir` (ou dÃ©faut : `figures/flex_<mode>_<lin|mlp>`), et les checkpoints dans `checkpoints/flex_<mode>`.

### Figures

* `V_star_heatmap.png` : heatmap de (V^*(s)=\max_a Q(s,a))
* `pi_star_grid.png` : politique (\pi^*(s)=\arg\max_a Q(s,a))
* `policy_value.png` : V* + Ï€*
* `visits.png` : heatmap des visites dâ€™Ã©tats
* `dominant_actions.png` : action dominante par case
* `summary_dashboard.png` : dashboard rÃ©capitulatif
* `naive_*.png` / `dqn_*.png` : `loss_mean_per_episode`, `steps_per_episode`, `epsilon_theta_over_episodes`, `loss_vs_return`

### CSV & checkpoints

* `<mode>_logs.csv` : `episode, return, length, mean_loss, epsilon, theta_norm`
* `checkpoints/flex_<mode>/best_<mode>.pt` et `final_<mode>.pt`

---

## 9) Protocole de comparaison (ce que je fais)

* **MÃªme seed**, **mÃªmes Ã©pisodes**, **mÃªme horizon `--max-steps`**, **mÃªmes rÃ©compenses**.
* NAÃF : `--hidden "" --lr 0.05` ; DQN : `--hidden "64,64" --lr 1e-3`.
* **MÃªme perte** (je prends **Huber** pour les deux) :

  * `--loss huber --huber-beta 1.0`
* Je compare **Ã©volution du return**, **perte moyenne par Ã©pisode**, **longueur dâ€™Ã©pisode**, **Îµ**, **â€–Î¸â€–**.

---

## 10) Performance & stabilitÃ© (mes repÃ¨res)

* Si jâ€™ai un GPU : `--device cuda`.
* DQN plus rapide si je descends `--batch 32`, `--hidden "32,32"`, ou si jâ€™augmente `--train-every 2`.
* **Huber** + clipping (dÃ©jÃ  activÃ©) = trÃ¨s bon compromis au dÃ©but de lâ€™apprentissage.
* **Backend Agg** : pas dâ€™UI â†’ plus rapide, plots Ã©crits directement.

---

## 11) DÃ©pannage rapide

* **Erreur one_hot / LongTensor** : lâ€™agent encode dÃ©sormais via une **matrice identitÃ©** et cast en `torch.long` ; je nâ€™ai plus ce souci.
* **Import circulaire** : vÃ©rifier que `agentDQN_flexible.py` nâ€™importe pas lui-mÃªme son propre nom.
* **PowerShell & `--hidden ""`** : si Ã§a casse, je mets `--hidden ""` entre guillemets doubles, ou bien jâ€™Ã©cris `--hidden none`.
* **Plots vides** : le dossier `figures/...` doit exister (crÃ©Ã© automatiquement avec `ensure_dir`), et `matplotlib.use("Agg")` est bien au tout dÃ©but.

---

## 12) Notes sur NAÃF â€œlinÃ©aireâ€ (clarification)

Quand jâ€™Ã©cris â€œlinÃ©aire (1 neurone)â€, je veux dire **â€œpas de hidden layerâ€** :

* **36 entrÃ©es** (one-hot) â†’ **4 sorties** (une par action).
* Câ€™est lâ€™Ã©quivalent dâ€™une **table Q** paramÃ©trique, entraÃ®nÃ©e par descente de gradient.
* DerniÃ¨re couche **sans activation** (les Q ne sont pas des probabilitÃ©s).

---

## 13) Exemples de lancement (rÃ©cap)

**NAÃF (linÃ©aire) â€” Huber, LR haut, 800 Ã©pisodes :**

```bash
python -m minegym.experiments.exp_flexible \
  --mode naive --hidden "" --episodes 800 --max-steps 120 --seed 0 \
  --lr 0.05 --loss huber --huber-beta 1.0
```

**DQN (64-64) â€” Huber, 800 Ã©pisodes :**

```bash
python -m minegym.experiments.exp_flexible \
  --mode dqn --hidden "64,64" --episodes 800 --max-steps 120 --seed 0 \
  --lr 1e-3 --loss huber --huber-beta 1.0
```

Les rÃ©compenses sont **imposÃ©es** dans la crÃ©ation de `GridEnv` (voir section 3).
Jâ€™analyse ensuite les figures et le CSV dans `figures/flex_<mode>_<lin|mlp>/` et `checkpoints/flex_<mode>/`.

---

Fin du README.
