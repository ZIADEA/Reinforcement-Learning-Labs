<div align="center">

<!-- BanniÃ¨re animÃ©e -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=2,5,8&height=150&section=header&text=Seance%204&fontSize=50&fontColor=fff&animation=fadeIn&fontAlignY=38&desc=GridWorld%20Flexible%20%26%20DQN&descAlignY=55&descAlign=50"/>

<br/>

![DQN](https://img.shields.io/badge/Deep_RL-DQN-red?style=for-the-badge&logo=pytorch)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge&logo=checkmarx)
![Neural](https://img.shields.io/badge/Neural_Networks-Implemented-3498db?style=for-the-badge)

<br/>

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=20&duration=3000&pause=1000&color=C0392B&center=true&vCenter=true&width=700&lines=Q-Learning+CorrigÃ©+%2B+Deep+Q-Networks;ğŸ¤–+Comparaison+NaÃ¯f+vs+DQN+Complet;ğŸ“ˆ+Architecture+Flexible+(LinÃ©aire+%2F+MLP)" alt="Typing SVG" />

---

## ğŸ“ Note PÃ©dagogique : La RÃ©volution du Deep Q-Network (DQN)

### ğŸŒ Le "Moment Atari" (2013-2015)
Si vous devez retenir une date dans l'histoire rÃ©cente de l'IA, c'est **2015**. C'est l'annÃ©e oÃ¹ l'Ã©quipe de **DeepMind** (Mnih et al.) a publiÃ© dans *Nature* leur article sur le **DQN**.

Jusqu'alors, le RL Ã©tait limitÃ© Ã  des problÃ¨mes "jouets" (comme notre GridWorld tabulaire) oÃ¹ l'on pouvait stocker chaque Ã©tat dans un tableau. Mais comment jouer Ã  un jeu vidÃ©o Atari oÃ¹ l'Ã©cran fait 210x160 pixels ? Le nombre d'Ã©tats possibles dÃ©passe le nombre d'atomes dans l'univers ($256^{210 \times 160}$). Le tableau Q est impossible Ã  stocker.

L'idÃ©e de gÃ©nie a Ã©tÃ© de remplacer le tableau Q par un **rÃ©seau de neurones profond** (Deep Neural Network) capable d'approximer la fonction $Q(s, a)$. L'agent ne "mÃ©morise" plus la grille, il "voit" et "comprend" l'image.

### âš ï¸ Le ProblÃ¨me de la "Triade Mortelle"
Pourquoi n'avions-nous pas fait cela plus tÃ´t ? Parce que combiner le RL avec des rÃ©seaux de neurones est notoirement instable. C'est ce que Sutton et Barto appellent la "Deadly Triad" (Triade Mortelle) :
1.  **Approximation de fonction** (RÃ©seaux de neurones)
2.  **Bootstrapping** (Utiliser une estimation pour mettre Ã  jour une autre estimation)
3.  **Off-policy training** (Apprendre sur des donnÃ©es gÃ©nÃ©rÃ©es par une ancienne politique)

Quand on mÃ©lange ces trois ingrÃ©dients naÃ¯vement, l'apprentissage diverge souvent vers l'infini. L'agent devient "fou".

### ğŸ› ï¸ Les Solutions Techniques du DQN
Dans cette sÃ©ance, nous implÃ©mentons les deux innovations majeures qui ont permis de stabiliser le DQN, transformant une idÃ©e instable en une rÃ©volution technologique :

1.  **Experience Replay (Replay Buffer)** :
    *   *ProblÃ¨me* : Dans un jeu, les donnÃ©es sont corrÃ©lÃ©es (l'image Ã  t+1 ressemble Ã  t). Les rÃ©seaux de neurones dÃ©testent Ã§a (ils oublient ce qu'ils ont appris avant).
    *   *Solution* : On stocke les transitions $(s, a, r, s')$ dans une mÃ©moire gÃ©ante et on s'entraÃ®ne sur un **lot alÃ©atoire** (batch). Cela brise les corrÃ©lations temporelles.

2.  **Target Network (RÃ©seau Cible)** :
    *   *ProblÃ¨me* : On essaie d'approcher une cible $r + \gamma \max Q(s', a')$. Mais $Q$ est le rÃ©seau lui-mÃªme ! C'est comme un chien qui court aprÃ¨s sa queue : la cible bouge Ã  chaque mise Ã  jour.
    *   *Solution* : On crÃ©e une copie "gelÃ©e" du rÃ©seau (Target Network) pour calculer la cible. On ne met Ã  jour cette copie que toutes les X Ã©tapes. La cible devient stable.

> **ğŸ“š RÃ©fÃ©rence Incontournable :**
> *Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.*

---

<br/>

<p align="center">
  <a href="README.md">
    <img src="https://img.shields.io/badge/ğŸ“–_README_Principal-4A90E2?style=for-the-badge"/>
  </a>
  <a href="DQNReadme.md">
    <img src="https://img.shields.io/badge/ğŸ§ª_DÃ©tails_DQN-EE4C2C?style=for-the-badge"/>
  </a>
</p>

</div>

<br/>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif">

<br/>

## ğŸ¯ AperÃ§u

Cette sÃ©ance poursuit le travail sur le GridWorld paramÃ©trable tout en s'assurant que chaque agent peut toujours gÃ©rer la nouvelle dÃ©finition du monde. L'accent est mis sur la **mise Ã  jour corrigÃ©e des valeurs d'action**, les nouvelles **mÃ©triques Q-Learning** et la **comparaison DQN vs naÃ¯f** dÃ©crite dans [`DQNReadme.md`](DQNReadme.md).

## ğŸš€ DÃ©marrage Rapide

<details open>
<summary><b>âš™ï¸ Configuration de l'Environnement</b></summary>

```powershell
& C:\Users\DJERI\VSCODE\Programmation\python\environnements\rl_venv\Scripts\Activate.ps1
```
</details>

<details open>
<summary><b>ğŸ“ Lancer les Diagnostics Q-Learning</b></summary>

```bash
cd Sceance4/minegym
python -m minegym.experiments.liveQL
python -m minegym.experiments.sensitivity_gammaQL
python -m minegym.experiments.sensitivity_grid_sizeQL
```
</details>

<details>
<summary><b>ğŸ¤– Lancer les ExpÃ©riences DQN</b></summary>

Pour l'expÃ©rience DQN flexible (`exp_flexible.py`) et la comparaison entre l'agent linÃ©aire naÃ¯f et la pile DQN complÃ¨te, voir la rÃ©fÃ©rence CLI complÃ¨te dans [ğŸ“– DQNReadme.md](DQNReadme.md).

</details>

### What changed in this sÃ©ance?

- The GridWorld now accepts dynamic goals, obstacles, and reward tweaks without touching the scripts.
- All figures (live animation, sensitivity plots, dashboards) still live in `figures/goal`, but the Q-Learning agentâ€™s logging now includes the new `w` correction term.
- We validate that Q-Learning keeps converging before running the DQN comparison.

### Corrected Q-Learning update

We modify the tabular update to include a weighting factor `w`, so the library entry follows:

$$Q(s,a) \leftarrow Q(s,a) + \alpha \cdot w \cdot \left[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right].$$

The parameter `w` allows us to dampen or accentuate the move toward the TD target when the environment changes from fixed to mobile goals, or when obstacles are reconfigured.

---

## ğŸ–¼ï¸ Galerie ComplÃ¨te des RÃ©sultats Visuels

<div align="center">

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=18&pause=1000&color=C0392B&center=true&vCenter=true&width=500&lines=Analyse+DQN+Flexible;Naive+vs+Deep+Architectures" alt="Typing SVG" />

</div>

### ğŸ¯ RÃ©sultats Globaux - Politique & Valeurs

<details open>
<summary><b>ğŸ§­ Politique Optimale & Fonction de Valeur</b></summary>

<table>
<tr>
<td align="center" width="33%">
<img src="figures/flex_naive_lin/V_star_heatmap.png" width="100%"/>
<br/><br/>
<b>ğŸŒ¡ï¸ Heatmap V*</b>
<br/>
<sub><i>Valeurs d'Ã©tats optimales apprises</i></sub>
</td>
<td align="center" width="33%">
<img src="figures/flex_naive_lin/pi_star_grid.png" width="100%"/>
<br/><br/>
<b>ğŸ¯ Politique Ï€* (Grid)</b>
<br/>
<sub><i>FlÃ¨ches directionnelles par Ã©tat</i></sub>
</td>
<td align="center" width="33%">
<img src="figures/flex_naive_lin/policy_value.png" width="100%"/>
<br/><br/>
<b>ğŸ”„ Policy + Value Overlay</b>
<br/>
<sub><i>Superposition Ï€* et V*</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse :**
- **V_star_heatmap** : Mode naive arrive Ã  apprendre un gradient de valeurs (valeurs Ã©levÃ©es prÃ¨s du goal, nÃ©gatives loin)
- **pi_star_grid** : Politique cohÃ©rente malgrÃ© l'absence de replay buffer â†’ convergence grÃ¢ce Ã  l'environnement simple (6Ã—6)
- **policy_value** : Validation que arg max Q(s,a) suit bien le gradient de V*

**âš ï¸ Limitation Naive** : Sur des environnements plus complexes, l'absence de replay buffer causerait une instabilitÃ© (corrÃ©lation temporelle des transitions).

</details>

### ğŸ“ˆ MÃ©triques d'EntraÃ®nement - Convergence & Diagnostics

<details>
<summary><b>ğŸ”¬ Analyses DÃ©taillÃ©es du Processus d'Apprentissage</b></summary>

#### ğŸ¢ Ã‰volution de la Loss MSE

<p align="center">
<img src="figures/flex_naive_lin/naive_loss_mean_per_episode.png" width="70%"/>
<br/>
<i>Loss moyennÃ©e par Ã©pisode avec moving average (MA50)</i>
</p>

**ğŸ“ Analyse :**
- DÃ©croissance typique de la TD-loss : dÃ©bute Ã©levÃ©e (~25-30) quand Q est initialisÃ© alÃ©atoirement
- Convergence vers ~5-10 aprÃ¨s 800 Ã©pisodes â†’ rÃ©sidu dÃ» aux transitions stochastiques (Îµ-greedy persiste)
- Pics occasionnels : changements de goal alÃ©atoires (1 goal par Ã©pisode) crÃ©ent des distributions non-stationnaires

**ğŸ’¡ UtilitÃ©** : Confirme que l'optimiseur converge (loss dÃ©croissante = TD-error diminue)

---

#### ğŸƒ Longueur des Ã‰pisodes

<p align="center">
<img src="figures/flex_naive_lin/naive_steps_per_episode.png" width="70%"/>
<br/>
<i>Nombre de steps par Ã©pisode (MA50)</i>
</p>

**ğŸ“ Analyse :**
- DÃ©but : ~30-40 steps (exploration alÃ©atoire sur grille 6Ã—6)
- Convergence : ~12-15 steps â†’ proche de l'optimal Manhattan distance
- Variance Ã©levÃ©e mÃªme aprÃ¨s convergence : due aux goals alÃ©atoires (certains proches, d'autres lointains)

**ğŸ’¡ UtilitÃ©** : MÃ©trique proxy de performance (moins de steps = politique plus directe vers goal)

---

#### âš–ï¸ Exploration (Îµ) vs Poids du RÃ©seau (||Î¸||)

<p align="center">
<img src="figures/flex_naive_lin/naive_epsilon_theta_over_episodes.png" width="70%"/>
<br/>
<i>Dual-axis plot : epsilon (bleu) & norme L2 des poids (orange)</i>
</p>

**ğŸ“ Analyse :**
- **Îµ (bleu)** : DÃ©croissance exponentielle classique (Îµâ‚€=1.0 â†’ Îµ_min=0.01)
- **||Î¸|| (orange)** : Croissance initiale rapide puis stabilisation autour de 8-10
  - InterprÃ©tation : Le rÃ©seau linÃ©aire apprend des poids non-nuls pour extraire les features (position x,y)
  - Stabilisation = convergence des poids (pas de divergence catastrophique)

**ğŸ’¡ UtilitÃ©** : 
- DÃ©tecte l'overfitting (si ||Î¸|| explose) ou underfitting (si ||Î¸|| reste proche de 0)
- Valide que l'exploration dÃ©croÃ®t bien pendant l'apprentissage

---

#### ğŸ”— CorrÃ©lation Loss vs Return

<p align="center">
<img src="figures/flex_naive_lin/naive_loss_vs_return.png" width="70%"/>
<br/>
<i>Scatter plot : chaque point = 1 Ã©pisode (x=return cumulÃ©, y=loss moyennÃ©e)</i>
</p>

**ğŸ“ Analyse :**
- **CorrÃ©lation nÃ©gative attendue** : Ã‰pisodes avec retours Ã©levÃ©s (proches du goal) â†’ loss faible (Q bien calibrÃ©e)
- **Cluster principal** : Return âˆˆ [35, 50], Loss âˆˆ [5, 15] â†’ rÃ©gime stable aprÃ¨s convergence
- **Outliers** : 
  - Return faible + Loss Ã©levÃ©e : DÃ©but d'entraÃ®nement (exploration alÃ©atoire)
  - Return Ã©levÃ© + Loss faible : Fin d'entraÃ®nement (exploitation optimale)

**ğŸ’¡ UtilitÃ©** : Diagnostic de la qualitÃ© de l'approximateur Q :
- Si loss Ã©levÃ©e malgrÃ© retours Ã©levÃ©s â†’ rÃ©seau sous-exprime la valeur (capacity insuffisante)
- Si loss faible malgrÃ© retours faibles â†’ rÃ©seau sur-optimiste (surestimation biais)

</details>

### ğŸ—ºï¸ Analyses Comportementales

<details>
<summary><b>ğŸ” Distribution Spatiale & Actions</b></summary>

<table>
<tr>
<td align="center" width="50%">
<img src="figures/flex_naive_lin/visits.png" width="100%"/>
<br/><br/>
<b>ğŸ—ºï¸ Heatmap des Visites</b>
<br/>
<sub><i>Ã‰tats visitÃ©s durant l'entraÃ®nement</i></sub>
</td>
<td align="center" width="50%">
<img src="figures/flex_naive_lin/dominant_actions.png" width="100%"/>
<br/><br/>
<b>ğŸ² Actions Dominantes Empiriques</b>
<br/>
<sub><i>Action la plus frÃ©quente par Ã©tat</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse :**
- **visits.png** : 
  - Distribution relativement uniforme (goals alÃ©atoires forcent exploration de tout l'espace)
  - Zones centrales lÃ©gÃ¨rement plus visitÃ©es (moyennement accessibles depuis n'importe quel dÃ©part)
  
- **dominant_actions.png** :
  - CohÃ©rence avec Ï€* (flÃ¨ches empiriques â‰ˆ politique apprise)
  - RÃ©vÃ¨le les biais stochastiques (certains Ã©tats favorisent une action Ã  cause de l'Îµ-greedy rÃ©siduel)

**ğŸ’¡ UtilitÃ©** : DÃ©tecte les zones sous-explorÃ©es (potentiels trous dans la coverage) ou sur-visitÃ©es (biais de l'exploration)

</details>

### ğŸ“Š Dashboard RÃ©capitulatif

<details>
<summary><b>ğŸ“ˆ Vue d'Ensemble 6-en-1</b></summary>

<p align="center">
<img src="figures/flex_naive_lin/summary_dashboard.png" width="90%"/>
<br/><br/>
<b>Dashboard Complet</b> : 6 subplots (reward distribution, convergence, episode length, action distribution, explore/exploit, reward/step)
</p>

**ğŸ“ Contenu du Dashboard :**

1. **Reward Distribution per Action** : Histogramme montrant que toutes les actions obtiennent des rewards similaires (environnement symÃ©trique)
2. **Convergence Curve** : Return cumulÃ© avec MA50 â†’ croissance vers +45 optimal
3. **Episode Length** : DÃ©croissance vers ~12 steps (optimal)
4. **Action Distribution** : Proportions des 4 actions (~25% chacune initialement, puis biais selon goal)
5. **Exploration vs Exploitation** : Ratio greedy actions augmente de 0% â†’ 99%
6. **Reward per Step** : Moyenne par Ã©pisode â†’ converge vers +3.5 par step (optimal = goal_reward / steps)

**ğŸ’¡ UtilitÃ©** : Vue synthÃ©tique pour diagnostiquer rapidement tout problÃ¨me (plateau prÃ©maturÃ©, biais d'action, exploration insuffisante, etc.)

</details>

---

### ğŸ“‚ Fichier CSV - Logs DÃ©taillÃ©s

Le fichier `naive_logs.csv` contient les mÃ©triques brutes par Ã©pisode :

```csv
episode,steps,return,loss,epsilon,theta_norm
0,42,-8.5,28.3,1.0,0.12
1,38,-5.2,25.1,0.995,0.87
...
999,13,48.7,6.2,0.01,9.14
```

**ğŸ“ Colonnes :**
- **episode** : NumÃ©ro de l'Ã©pisode (0-999)
- **steps** : Longueur de l'Ã©pisode
- **return** : Retour cumulÃ© G_t
- **loss** : TD-loss MSE moyennÃ©e sur l'Ã©pisode
- **epsilon** : Valeur d'Îµ pour cet Ã©pisode
- **theta_norm** : Norme L2 des poids du rÃ©seau (||Î¸||â‚‚)

**ğŸ’¡ UtilitÃ©** : 
- Export pour analyses externes (Pandas, Excel, TensorBoard)
- ReproductibilitÃ© exacte des rÃ©sultats
- Calcul de statistiques personnalisÃ©es (variance inter-runs, correlation matrix, etc.)

---

## ğŸ“Š RÃ©sumÃ© Visual Assets SÃ©ance 4

<div align="center">

| CatÃ©gorie | Images | CSV | Total | UtilitÃ© Principale |
|-----------|--------|-----|-------|-------------------|
| **Politique & Valeur** | 3 | 0 | 3 | Validation apprentissage optimal |
| **MÃ©triques Training** | 4 | 0 | 4 | Convergence & diagnostics |
| **Analyses Spatiales** | 2 | 0 | 2 | Distribution exploration |
| **Dashboard** | 1 | 0 | 1 | Vue synthÃ©tique |
| **Logs** | 0 | 1 | 1 | DonnÃ©es brutes export |
| **TOTAL** | **10** | **1** | **11** | **Analyse complÃ¨te DQN Naive** |

</div>

**ğŸ” Comparaison Naive vs DQN (prÃ©vu) :**
- Mode `--mode dqn` gÃ©nÃ¨re les mÃªmes visualisations dans `figures/flex_dqn/`
- Comparaison attendue :
  - **Loss** : DQN plus stable (replay buffer dÃ©corrÃ¨le transitions)
  - **Convergence** : DQN plus rapide (target network rÃ©duit moving target problem)
  - **Performance finale** : Similaire sur gridworld 6Ã—6 (trop simple pour voir l'avantage DQN)

---

## ğŸ“š Guides de Modification de l'Environnement

### âš™ï¸ Fichier Principal

Les actions sont codÃ©es dans `GridEnv` via `action_space = 4` et les mouvements dans `step()` :

```python
# 0: gauche ; 1: droite ; 2: haut ; 3: bas ;c:colonne actuel ; r:ligne actuel ; pc , pr : coordonnees qui devrai etrer obtenu apres l action sans tenir compt des limites du grid
if action == 0: pc = c - 1
elif action == 1: pc = c + 1
elif action == 2: pr = r - 1
elif action == 3: pr = r + 1
# r = state // self.cols   
# c = state %  self.cols 
```
  


Les Ã©tats sont simplement les entiers `0 â€¦ rows*cols-1` (index ligne-colonne aplati).


## Modifier les rÃ©compenses (rewards) : 
Dans la classe `GridEnv.__init__` :

```python
self.reward_step = -3 #for waste of time
self.reward_obstacle_attempt = -10 #for obstacle rebump
self.reward_wall_bump = -5 #for limit of grid rebump
self.reward_goal = +35 #for goal
```

## Obstacles (nombre et position)

Dans les fichier de `experiments\` lors de l'instanciation de l env on passe une liste d'indices  representant la position des obstacles ce qui definit leur nombres et leurs position  :
-Nombre d'obstacles : longueur de la liste
-Position : positionner a indice = row*cols + col ( equation provenant de la division euclidienne index par cols oÃ¹ row = index // cols et col = index % cols )

```python
obstacles = [7, 8, 13]
env = GridEnv(rows=6, cols=6, obstacles=obstacles, ...)
```

## Goals statiques (nombre et position)
Toujours lors de l'instanciation du env Dans les fichier de `experiments\` :
- Nombre de goals : taille de la liste
- Position : mÃªme logique que les obstacles (row*cols+col)

et il faut aussi garder 
```python
goals = [35]                 # un seul goal
goals = [35, 18, 5]          # plusieurs goals
env = GridEnv(..., goals=goals, ...)
```

## Goals dynamiques (bouger le goal Ã  chaque step)

Toujours lors de l'instanciation du env Dans les fichier de `experiments\` :
- Nombre de goals : taille de la liste
- Position de depart : mÃªme logique que les obstacles (row*cols+col) 

et il faut aussi garder 
```python
goals = [35]                 # un seul goal
goals = [35, 18, 5]          # plusieurs goals
env = GridEnv(..., goals=goals, ... , moving_goal=True,moving_mode="random") # ou moving_mode="cyclic" ou redefinir une autre methode dans un nouvelle valeur de moving_mode  dans _move_goals dans la classe GridEnv
```

#  graphiques gÃ©nÃ©rÃ©s dans `figures/`

## `experiments/liveQL.py`

**Pendant l'exÃ©cution** :  Le flux animÃ©  contient : 

- **gauche** : grille du monde (blanc=libre, noir=obstacle, vert=goal, rouge=agent)
- **Haut droite** : cumul du reward au fur et Ã  mesure du step courant
- **Bas droite** : reward total par Ã©pisode (avec moyenne mobile sur 10 Ã©pisodes)

on genere aussi :

## Pendant lâ€™exÃ©cution (fenÃªtre live)

- **Gauche** : grille (blanc=libre, noir=obstacle, vert=goal, rouge=agent, quadrillage visible).
- **Haut-droite** : cumul du reward du step courant.
- **Bas-droite** : reward par Ã©pisode + moyenne mobile(10).

- **live_training.mp4** â€” animation du live (H.264, frame-skip & downscale pour Ã©conomiser RAM).
- **V_star_heatmap_annotated.png** â€” heatmap annotÃ©e de V* (= max_a Q(s,a)).
- **pi_star_grid.png** â€” politique greedy (flÃ¨ches argmax_a Q(s,a)).
- **policy_value.png** â€” rÃ©cap valeur/politique.
- **visits.png** â€” heatmap des visites dâ€™Ã©tats.
- **dominant_actions.png** â€” action dominante observÃ©e par Ã©tat.
- **summary_dashboard.png** â€” dashboard rÃ©capitulatif (voir calculs).
- **live_explore_exploit_empirical.png** â€” Exploration/Exploitation : empirique vs thÃ©orique.


## `experiments/sensitivity_gamma.py`

- `sensitivity_gamma_convergence_ci.png` : **convergence** (reward moyen par Ã©pisode, **MA=50**), une courbe par Î³ (moyenne inter-seeds, IC95% optionnelle).
- `sensitivity_gamma_time_to_threshold.png` : **temps pour atteindre un seuil** de performance (MA=50 â‰¥ **-10**) â€” **plus bas = plus rapide**.
- `sensitivity_gamma_final.png` : **performance finale** (moyenne des **200** derniers Ã©pisodes) avec barres dâ€™erreur (Ã©cart-type inter-seeds).
- `sensitivity_gamma_episode_length.png` : **longueur des Ã©pisodes** (lissÃ© MA=50) vs Ã©pisodes pour **un seed fixe** (illustratif).
- `sensitivity_gamma_explore_exploit.png` : **profil exploration/exploitation thÃ©orique** (planification dâ€™Îµ via `eps_schedule`).
- `sensitivity_gamma_prop_greedy_subplots.png` : **profil exploration/exploitation empirique** (**proportion dâ€™actions greedy** mesurÃ©e pendant lâ€™apprentissage), 11 sous-figures (une par Î³).




## `experiments/sensitivity_grid_size.py`

- `sensitivity_grid_convergence.png` : **convergence** (reward moyen par Ã©pisode, **MA=50**), une courbe par **taille** de grille (4Ã—4 â†’ 10Ã—10).
- `sensitivity_grid_final.png` : **performance finale** (moyenne des **200** derniers Ã©pisodes) par **taille**, avec barres dâ€™erreur (Ã©cart-type intra-run).
- `sensitivity_grid_episode_length.png` : **longueur des Ã©pisodes** (lissÃ©e MA=50) vs Ã©pisodes, une courbe par **taille**.
- `sensitivity_grid_explore_exploit.png` : **profil exploration/exploitation thÃ©orique** (planification dâ€™Îµ commune Ã  toutes les tailles).
- `sensitivity_grid_prop_greedy_subplots.png` : **exploration/exploitation empirique** â€” 7 sous-figures (**proportion dâ€™actions greedy** mesurÃ©e pendant lâ€™apprentissage), une par **taille**.


# Note

- Toutes les valeurs (rÃ©compenses, obstacles, goals, start ) doivent Ãªtre modifiÃ©es dans les **scripts experiments/**  via en fonction de ce que on veut tester ou observer (goal mobile/fixe ; position des obstacles ,.....  ): 
```python
class GridEnv(
    rows: int = 4,
    cols: int = 4,
    obstacles: List[int] | None = None,
    goals: int | List[int] | None = None,
    start: int | None = None,
    seed: int = 0,
    reward_step: int = -3,
    reward_obstacle_attempt: int = -10,
    reward_wall_bump: int = -5,
    reward_goal: int = +20,
    moving_goal: bool = False,
    moving_mode: str = "random"
)
```

- sauf si tu veux changer dÃ©finitivement la logique â†’ alors modifie `gridworld.py`.




