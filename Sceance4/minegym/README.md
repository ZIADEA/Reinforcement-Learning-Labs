<div align="center">

<!-- Banni√®re anim√©e -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=2,5,8&height=150&section=header&text=Seance%204&fontSize=50&fontColor=fff&animation=fadeIn&fontAlignY=38&desc=GridWorld%20Flexible%20%26%20DQN&descAlignY=55&descAlign=50"/>

<br/>

![DQN](https://img.shields.io/badge/Deep_RL-DQN-red?style=for-the-badge&logo=pytorch)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge&logo=checkmarx)
![Neural](https://img.shields.io/badge/Neural_Networks-Implemented-3498db?style=for-the-badge)

<br/>

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=20&duration=3000&pause=1000&color=C0392B&center=true&vCenter=true&width=700&lines=Q-Learning+Corrig√©+%2B+Deep+Q-Networks;ü§ñ+Comparaison+Na√Øf+vs+DQN+Complet;üìà+Architecture+Flexible+(Lin√©aire+%2F+MLP)" alt="Typing SVG" />

<br/>

<p align="center">
  <a href="README.md">
    <img src="https://img.shields.io/badge/üìñ_README_Principal-4A90E2?style=for-the-badge"/>
  </a>
  <a href="DQNReadme.md">
    <img src="https://img.shields.io/badge/üß™_D√©tails_DQN-EE4C2C?style=for-the-badge"/>
  </a>
</p>

</div>

<br/>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif">

<br/>

## üéØ Aper√ßu

Cette s√©ance poursuit le travail sur le GridWorld param√©trable tout en s'assurant que chaque agent peut toujours g√©rer la nouvelle d√©finition du monde. L'accent est mis sur la **mise √† jour corrig√©e des valeurs d'action**, les nouvelles **m√©triques Q-Learning** et la **comparaison DQN vs na√Øf** d√©crite dans [`DQNReadme.md`](DQNReadme.md).

## üöÄ D√©marrage Rapide

<details open>
<summary><b>‚öôÔ∏è Configuration de l'Environnement</b></summary>

```powershell
& C:\Users\DJERI\VSCODE\Programmation\python\environnements\rl_venv\Scripts\Activate.ps1
```
</details>

<details open>
<summary><b>üìù Lancer les Diagnostics Q-Learning</b></summary>

```bash
cd Sceance4/minegym
python -m minegym.experiments.liveQL
python -m minegym.experiments.sensitivity_gammaQL
python -m minegym.experiments.sensitivity_grid_sizeQL
```
</details>

<details>
<summary><b>ü§ñ Lancer les Exp√©riences DQN</b></summary>

Pour l'exp√©rience DQN flexible (`exp_flexible.py`) et la comparaison entre l'agent lin√©aire na√Øf et la pile DQN compl√®te, voir la r√©f√©rence CLI compl√®te dans [üìñ DQNReadme.md](DQNReadme.md).

</details>

### What changed in this s√©ance?

- The GridWorld now accepts dynamic goals, obstacles, and reward tweaks without touching the scripts.
- All figures (live animation, sensitivity plots, dashboards) still live in `figures/goal`, but the Q-Learning agent‚Äôs logging now includes the new `w` correction term.
- We validate that Q-Learning keeps converging before running the DQN comparison.

### Corrected Q-Learning update

We modify the tabular update to include a weighting factor `w`, so the library entry follows:

$$Q(s,a) \leftarrow Q(s,a) + \alpha \cdot w \cdot \left[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right].$$

The parameter `w` allows us to dampen or accentuate the move toward the TD target when the environment changes from fixed to mobile goals, or when obstacles are reconfigured.

---

## üñºÔ∏è Galerie de Sorties Visuelles

<div align="center">

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=18&pause=1000&color=C0392B&center=true&vCenter=true&width=500&lines=R√©sultats+DQN+Na√Øf+(Lin√©aire);Architectures+Compar√©es" alt="Typing SVG" />

<br/><br/>

<table>
<tr>
<td align="center" width="33%">
<a href="figures/flex_naive_lin/V_star_heatmap.png">
<img src="figures/flex_naive_lin/V_star_heatmap.png" width="250" style="border: 3px solid #3498db; border-radius: 8px;"/>
</a>
<br/><br/>
<img src="https://img.shields.io/badge/Value_Function-3498db?style=flat-square&logo=python"/>
</td>
<td align="center" width="33%">
<a href="figures/flex_naive_lin/pi_star_grid.png">
<img src="figures/flex_naive_lin/pi_star_grid.png" width="250" style="border: 3px solid #2ecc71; border-radius: 8px;"/>
</a>
<br/><br/>
<img src="https://img.shields.io/badge/Optimal_Policy-2ecc71?style=flat-square&logo=checkmarx"/>
</td>
<td align="center" width="33%">
<a href="figures/flex_naive_lin/visits.png">
<img src="figures/flex_naive_lin/visits.png" width="250" style="border: 3px solid #e67e22; border-radius: 8px;"/>
</a>
<br/><br/>
<img src="https://img.shields.io/badge/State_Visits-e67e22?style=flat-square&logo=chartdotjs"/>
</td>
</tr>
<tr>
<td align="center" colspan="3">
<br/>
<a href="figures/flex_naive_lin/summary_dashboard.png">
<img src="figures/flex_naive_lin/summary_dashboard.png" width="800" style="border: 4px solid #9b59b6; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"/>
</a>
<br/><br/>
<img src="https://img.shields.io/badge/üìä_Complete_Dashboard-Full_Metrics-9b59b6?style=for-the-badge"/>
</td>
</tr>
</table>

</div>

---

### üìä Sorties Visuelles

- Live dashboards: `live_training.mp4`, `summary_dashboard.png`, reward and Œµ curves (see the `liveQL` description below).
- Heatmap artifacts: `V_star_heatmap_annotated.png`, `pi_star_grid.png`, `policy_value.png`, `visits.png`, `dominant_actions.png`.
- Sensitivity figures: each `sensitivity_gamma*` and `sensitivity_grid*` plot illustrates the convergence dynamics referred to in the table below.

# Modifier l'environnement GridEnv

Le fichier principal pour r√©gler la logique du monde est :`minegym/envs/gridworld.py`
## D√©finition des actions et √©tats

Les actions sont cod√©es dans `GridEnv` via `action_space = 4` et les mouvements dans `step()` :

```python
# 0: gauche ; 1: droite ; 2: haut ; 3: bas ;c:colonne actuel ; r:ligne actuel ; pc , pr : coordonnees qui devrai etrer obtenu apres l action sans tenir compt des limites du grid
if action == 0: pc = c - 1
elif action == 1: pc = c + 1
elif action == 2: pr = r - 1
elif action == 3: pr = r + 1
# r = state // self.cols   
# c = state %  self.cols 
```
  


Les √©tats sont simplement les entiers `0 ‚Ä¶ rows*cols-1` (index ligne-colonne aplati).


## Modifier les r√©compenses (rewards) : 
Dans la classe `GridEnv.__init__` :

```python
self.reward_step = -3 #for waste of time
self.reward_obstacle_attempt = -10 #for obstacle rebump
self.reward_wall_bump = -5 #for limit of grid rebump
self.reward_goal = +35 #for goal
```

## Obstacles (nombre et position)

Dans les fichier de `experiments\` lors de l'instanciation de l env on passe une liste d'indices  representant la position des obstacles ce qui definit leur nombres et leurs position  :
-Nombre d'obstacles : longueur de la liste
-Position : positionner a indice = row*cols + col ( equation provenant de la division euclidienne index par cols o√π row = index // cols et col = index % cols )

```python
obstacles = [7, 8, 13]
env = GridEnv(rows=6, cols=6, obstacles=obstacles, ...)
```

## Goals statiques (nombre et position)
Toujours lors de l'instanciation du env Dans les fichier de `experiments\` :
- Nombre de goals : taille de la liste
- Position : m√™me logique que les obstacles (row*cols+col)

et il faut aussi garder 
```python
goals = [35]                 # un seul goal
goals = [35, 18, 5]          # plusieurs goals
env = GridEnv(..., goals=goals, ...)
```

## Goals dynamiques (bouger le goal √† chaque step)

Toujours lors de l'instanciation du env Dans les fichier de `experiments\` :
- Nombre de goals : taille de la liste
- Position de depart : m√™me logique que les obstacles (row*cols+col) 

et il faut aussi garder 
```python
goals = [35]                 # un seul goal
goals = [35, 18, 5]          # plusieurs goals
env = GridEnv(..., goals=goals, ... , moving_goal=True,moving_mode="random") # ou moving_mode="cyclic" ou redefinir une autre methode dans un nouvelle valeur de moving_mode  dans _move_goals dans la classe GridEnv
```

#  graphiques g√©n√©r√©s dans `figures/`

## `experiments/liveQL.py`

**Pendant l'ex√©cution** :  Le flux anim√©  contient : 

- **gauche** : grille du monde (blanc=libre, noir=obstacle, vert=goal, rouge=agent)
- **Haut droite** : cumul du reward au fur et √† mesure du step courant
- **Bas droite** : reward total par √©pisode (avec moyenne mobile sur 10 √©pisodes)

on genere aussi :

## Pendant l‚Äôex√©cution (fen√™tre live)

- **Gauche** : grille (blanc=libre, noir=obstacle, vert=goal, rouge=agent, quadrillage visible).
- **Haut-droite** : cumul du reward du step courant.
- **Bas-droite** : reward par √©pisode + moyenne mobile(10).

- **live_training.mp4** ‚Äî animation du live (H.264, frame-skip & downscale pour √©conomiser RAM).
- **V_star_heatmap_annotated.png** ‚Äî heatmap annot√©e de V* (= max_a Q(s,a)).
- **pi_star_grid.png** ‚Äî politique greedy (fl√®ches argmax_a Q(s,a)).
- **policy_value.png** ‚Äî r√©cap valeur/politique.
- **visits.png** ‚Äî heatmap des visites d‚Äô√©tats.
- **dominant_actions.png** ‚Äî action dominante observ√©e par √©tat.
- **summary_dashboard.png** ‚Äî dashboard r√©capitulatif (voir calculs).
- **live_explore_exploit_empirical.png** ‚Äî Exploration/Exploitation : empirique vs th√©orique.


## `experiments/sensitivity_gamma.py`

- `sensitivity_gamma_convergence_ci.png` : **convergence** (reward moyen par √©pisode, **MA=50**), une courbe par Œ≥ (moyenne inter-seeds, IC95% optionnelle).
- `sensitivity_gamma_time_to_threshold.png` : **temps pour atteindre un seuil** de performance (MA=50 ‚â• **-10**) ‚Äî **plus bas = plus rapide**.
- `sensitivity_gamma_final.png` : **performance finale** (moyenne des **200** derniers √©pisodes) avec barres d‚Äôerreur (√©cart-type inter-seeds).
- `sensitivity_gamma_episode_length.png` : **longueur des √©pisodes** (liss√© MA=50) vs √©pisodes pour **un seed fixe** (illustratif).
- `sensitivity_gamma_explore_exploit.png` : **profil exploration/exploitation th√©orique** (planification d‚ÄôŒµ via `eps_schedule`).
- `sensitivity_gamma_prop_greedy_subplots.png` : **profil exploration/exploitation empirique** (**proportion d‚Äôactions greedy** mesur√©e pendant l‚Äôapprentissage), 11 sous-figures (une par Œ≥).




## `experiments/sensitivity_grid_size.py`

- `sensitivity_grid_convergence.png` : **convergence** (reward moyen par √©pisode, **MA=50**), une courbe par **taille** de grille (4√ó4 ‚Üí 10√ó10).
- `sensitivity_grid_final.png` : **performance finale** (moyenne des **200** derniers √©pisodes) par **taille**, avec barres d‚Äôerreur (√©cart-type intra-run).
- `sensitivity_grid_episode_length.png` : **longueur des √©pisodes** (liss√©e MA=50) vs √©pisodes, une courbe par **taille**.
- `sensitivity_grid_explore_exploit.png` : **profil exploration/exploitation th√©orique** (planification d‚ÄôŒµ commune √† toutes les tailles).
- `sensitivity_grid_prop_greedy_subplots.png` : **exploration/exploitation empirique** ‚Äî 7 sous-figures (**proportion d‚Äôactions greedy** mesur√©e pendant l‚Äôapprentissage), une par **taille**.


# Note

- Toutes les valeurs (r√©compenses, obstacles, goals, start ) doivent √™tre modifi√©es dans les **scripts experiments/**  via en fonction de ce que on veut tester ou observer (goal mobile/fixe ; position des obstacles ,.....  ): 
```python
class GridEnv(
    rows: int = 4,
    cols: int = 4,
    obstacles: List[int] | None = None,
    goals: int | List[int] | None = None,
    start: int | None = None,
    seed: int = 0,
    reward_step: int = -3,
    reward_obstacle_attempt: int = -10,
    reward_wall_bump: int = -5,
    reward_goal: int = +20,
    moving_goal: bool = False,
    moving_mode: str = "random"
)
```

- sauf si tu veux changer d√©finitivement la logique ‚Üí alors modifie `gridworld.py`.




