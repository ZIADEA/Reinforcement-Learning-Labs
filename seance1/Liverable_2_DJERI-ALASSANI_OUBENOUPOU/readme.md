<div align="center">

# ğŸ“– Livrable 2 â€” Agents Fondamentaux RL

![MC](https://img.shields.io/badge/Monte_Carlo-4169E1?style=for-the-badge)
![DP](https://img.shields.io/badge/Dynamic_Programming-2ECC71?style=for-the-badge)
![QL](https://img.shields.io/badge/Q--Learning-E67E22?style=for-the-badge)
![PI](https://img.shields.io/badge/Policy_Iteration-9B59B6?style=for-the-badge)

**ImplÃ©mentation complÃ¨te des 4 algorithmes fondamentaux de Reinforcement Learning**

</div>

---

## ğŸ¯ Contenu

Ce livrable contient les **implÃ©mentations de base** des algorithmes RL fondamentaux :

<table>
<tr>
<th>Fichier</th>
<th>Description</th>
<th>Algorithme</th>
</tr>
<tr>
<td><code>agentMC.py</code></td>
<td>Agent Monte Carlo</td>
<td>Apprentissage par Ã©pisodes complets avec moyenne des retours</td>
</tr>
<tr>
<td><code>agentPI.py</code></td>
<td>Agent Policy Iteration</td>
<td>AmÃ©lioration itÃ©rative de politique avec Ã©valuation complÃ¨te</td>
</tr>
<tr>
<td><code>agentVI.py</code></td>
<td>Agent Value Iteration</td>
<td>Convergence vers fonction de valeur optimale</td>
</tr>
<tr>
<td><code>agentQL.py</code></td>
<td>Agent Q-Learning</td>
<td>Apprentissage par diffÃ©rence temporelle (TD)</td>
</tr>
<tr>
<td><code>4agenttest.py</code></td>
<td>Script de test</td>
<td>ExÃ©cution et comparaison des 4 agents</td>
</tr>
<tr>
<td><code>gym1.py</code></td>
<td>Environnement</td>
<td>Environnement Gym personnalisÃ© pour les tests</td>
</tr>
</table>

## ğŸš€ Utilisation

### Installation

```bash
cd seance1/Liverable_2_DJERI-ALASSANI_OUBENOUPOU
```

### Lancer les Tests

```bash
python 4agenttest.py
```

Ce script va :
1. âœ… Charger l'environnement `gym1.py`
2. âœ… EntraÃ®ner les 4 agents (MC, PI, VI, Q-Learning)
3. âœ… Comparer leurs performances
4. âœ… Afficher les rÃ©sultats

## ğŸ§  Algorithmes DÃ©taillÃ©s

### ğŸ”µ Monte Carlo (MC)
- **Principe** : Apprentissage Ã  partir d'Ã©pisodes complets
- **MÃ©thode** : Moyenne des retours observÃ©s
- **Avantage** : Pas besoin du modÃ¨le de l'environnement
- **Fichier** : `agentMC.py`

### ğŸŸ¢ Dynamic Programming (PI/VI)
- **Policy Iteration** : Ã‰valuation puis amÃ©lioration de politique
- **Value Iteration** : Mise Ã  jour directe vers V*
- **Avantage** : Convergence garantie
- **Fichiers** : `agentPI.py`, `agentVI.py`

### ğŸŸ  Q-Learning
- **Principe** : Temporal Difference (TD)
- **Formule** : Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
- **Avantage** : Apprentissage en ligne, sans modÃ¨le
- **Fichier** : `agentQL.py`

## ğŸ“Š RÃ©sultats Attendus

AprÃ¨s exÃ©cution de `4agenttest.py`, vous devriez observer :
- âœ… Convergence de chaque algorithme
- âœ… Politiques optimales similaires
- âœ… DiffÃ©rences de vitesse de convergence
- âœ… Comparaison des performances

## ğŸ”§ ParamÃ¨tres ClÃ©s

| ParamÃ¨tre | Description | Valeur Typique |
|-----------|-------------|----------------|
| `alpha` (Î±) | Taux d'apprentissage | 0.1 - 0.5 |
| `gamma` (Î³) | Facteur d'escompte | 0.9 - 0.99 |
| `epsilon` (Îµ) | Taux d'exploration | 0.1 - 0.3 |
| `episodes` | Nombre d'Ã©pisodes | 1000 - 5000 |

## ğŸ“ Notes

- Ce livrable reprÃ©sente les **implÃ©mentations de base** des algorithmes
- Voir **Livrable 3** pour la version Ã©tendue avec amÃ©liorations
- L'environnement `gym1.py` est simple pour faciliter l'apprentissage

---

<div align="center">

**[â† Retour Seance 1](../README.md)** | **[Livrable 3 â†’](../Liverable_3_DJERI-ALASSANI_OUBENOUPOU)**

</div>
