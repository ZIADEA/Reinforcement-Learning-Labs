<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=140&section=header&text=Analyse%20Visuelle%20Compl√®te&fontSize=42&fontColor=fff&animation=fadeIn&fontAlignY=38&desc=Catalogue%20des%2063%20Fichiers%20Visuels&descAlignY=55&descAlign=50"/>

<br/>

![Images](https://img.shields.io/badge/Images_PNG-45-3498db?style=for-the-badge&logo=files)
![GIFs](https://img.shields.io/badge/GIFs-8-2ecc71?style=for-the-badge&logo=giphy)
![CSV](https://img.shields.io/badge/CSV_Logs-~10-f39c12?style=for-the-badge&logo=databricks)

**Document r√©capitulatif de tous les assets visuels avec analyses et justifications**

</div>

<br/>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif">

## üìã Table des Mati√®res

1. [S√©ance 2 - GridWorld Q-Learning](#seance-2---gridworld-q-learning-27-png--1-csv)
2. [S√©ance 4 - DQN Flexible](#seance-4---dqn-flexible-10-png--1-csv)
3. [S√©ance 5 - PPO Stable-Baselines3](#seance-5---ppo-stable-baselines3-8-gifs--8-png--8-csv)
4. [R√©capitulatif Global](#recapitulatif-global)

---

## S√©ance 2 - GridWorld Q-Learning (27 PNG + 1 CSV)

### üìÇ `liveQLgoalsfixed/` (7 fichiers)

| # | Fichier | Type | Dimensions | Description | Utilit√© |
|---|---------|------|-----------|-------------|----------|
| 1 | `V_star_heatmap_annotated.png` | PNG | ~800√ó600 | **Heatmap de V\*** avec annotations (goal, obstacles, valeurs num√©riques) | Visualiser la fonction de valeur optimale : zones chaudes = proches du goal, froides = loin. Valide que l'agent a appris la structure de r√©compense. |
| 2 | `pi_star_grid.png` | PNG | ~800√ó600 | **Politique optimale œÄ\*** (grille de fl√®ches ‚Üê‚Üí‚Üë‚Üì) | V√©rifier que chaque fl√®che pointe vers le goal. D√©tecte les incoh√©rences (fl√®ches vers obstacles). |
| 3 | `policy_value.png` | PNG | ~1000√ó600 | **Combinaison œÄ\* + V\*** (overlay fl√®ches + heatmap) | Vue unifi√©e : valide que arg max Q(s,a) extrait bien l'action qui maximise V*. |
| 4 | `visits.png` | PNG | ~800√ó600 | **Heatmap des visites d'√©tats** (nb de fois o√π agent passe par chaque case) | Identifier biais d'exploration : zones sur-explor√©es vs n√©glig√©es. Utile pour d√©tecter si Œµ-greedy couvre uniform√©ment l'espace. |
| 5 | `dominant_actions.png` | PNG | ~800√ó600 | **Action dominante empirique** (action la + fr√©quente par √©tat) | Compl√©ment √† œÄ* : r√©v√®le le comportement empirique vs th√©orique (Œµ-greedy r√©siduel biaise certaines actions). |
| 6 | `summary_dashboard.png` | PNG | ~1600√ó1200 | **Dashboard 6-en-1** : reward distribution, convergence, episode length, action distribution, explore/exploit, reward/step | **Vue synth√©tique compl√®te** : diagnostic rapide de 6 m√©triques cl√©s. Identifie probl√®mes (plateau, biais action, exploration insuffisante). |
| 7 | `live_explore_exploit_empirical.png` | PNG | ~900√ó600 | **Proportion actions greedy** (empirique vs th√©orique Œµ) | Valide que l'agent respecte le schedule d'Œµ (courbe empirique ‚âà th√©orique). D√©tecte si stochastique de l'env force plus d'exploration. |

**üìù G√©n√©ration** : Script `liveQL.py` ‚Äî entra√Æne Q-Learning avec dashboard anim√©, g√©n√®re MP4 + 7 PNGs diagnostiques.

---

### üìÇ `sensitivity_gamma/` (6 fichiers)

| # | Fichier | Type | Description | Utilit√© |
|---|---------|------|-------------|----------|
| 8 | `sensitivity_gamma_convergence_ci.png` | PNG | **Courbes convergence** pour Œ≥ ‚àà [0.0, 0.99] (11 valeurs), MA50, avec intervalles de confiance (5 seeds) | **Comparer vitesse et qualit√© finale** selon Œ≥ : Œ≥ faibles ‚Üí rapide mais sous-optimal ; Œ≥ √©lev√©s ‚Üí lent mais optimal. Guide choix hyperparam√®tre. |
| 9 | `sensitivity_gamma_time_to_threshold.png` | PNG | **Barplot temps pour atteindre seuil** (MA50 ‚â• -10) par Œ≥ | Quantifier "quelle Œ≥ converge le plus vite ?". R√©v√®le que Œ≥=0.5-0.7 optimal pour vitesse. |
| 10 | `sensitivity_gamma_final.png` | PNG | **Performance finale** (moyenne 200 derniers √©pisodes) avec barres d'erreur | Confirme que Œ≥=0.99 atteint ~+40 (optimal) vs Œ≥=0.1 plafonne √† ~-5. |
| 11 | `sensitivity_gamma_episode_length.png` | PNG | **Longueur √©pisodes** (MA50) vs √©pisodes, pour 1 seed illustratif | Montre que Œ≥ √©lev√©s ‚Üí trajectoires + longues initialement (exploration profonde) puis stabilisation. |
| 12 | `sensitivity_gamma_explore_exploit.png` | PNG | **Profil Œµ th√©orique** (planification epsilon_schedule) | Baseline : tous suivent m√™me d√©croissance d'Œµ (diff√©rences viennent du Œ≥, pas de l'exploration). |
| 13 | `sensitivity_gamma_prop_greedy_subplots.png` | PNG | **11 subplots** (1 par Œ≥) : proportion actions greedy empirique vs √©pisodes | R√©v√®le que Œ≥=0.0 reste <80% greedy (politique instable), autres convergent >90%. |

**üìù G√©n√©ration** : Script `sensitivity_gammaQL.py` ‚Äî sweep Œ≥ avec 5 seeds, 2000 √©pisodes, grille 10√ó10.

---

### üìÇ `sensitivity_grid_size/` (5 fichiers)

| # | Fichier | Type | Description | Utilit√© |
|---|---------|------|-------------|----------|
| 14 | `sensitivity_grid_convergence.png` | PNG | **Courbes convergence** pour tailles 4√ó4, 6√ó6, 8√ó8, 10√ó10 | Quantifier scaling laws : 4√ó4 converge en 100 ep, 10√ó10 en 1000 ep ‚Üí scaling ~quadratique. |
| 15 | `sensitivity_grid_final.png` | PNG | **Performance finale** par taille | Confirme reward final identique (~+40) car envs normalis√©s (m√™me goal_reward ratio). |
| 16 | `sensitivity_grid_episode_length.png` | PNG | **Longueur √©pisodes** par taille | Croissance lin√©aire : 4√ó4 ~8 steps, 10√ó10 ~25 steps (distance Manhattan augmente). |
| 17 | `sensitivity_grid_explore_exploit.png` | PNG | **Profil Œµ th√©orique** commun | Baseline : m√™me schedule pour toutes tailles (diff√©rences = complexit√© state space). |
| 18 | `sensitivity_grid_prop_greedy_subplots.png` | PNG | **7 subplots** (1 par taille) : proportion greedy | 4√ó4 atteint 95% d√®s ep 200, 10√ó10 vers ep 800 ‚Üí exploration + longue sur grands espaces. |

**üìù G√©n√©ration** : Script `sensitivity_grid_sizeQL.py` ‚Äî teste 7 tailles avec obstacles diagonaux.

---

### üìÇ `exp_td0_linear/` (3 PNG + 1 CSV)

| # | Fichier | Type | Description | Utilit√© |
|---|---------|------|-------------|----------|
| 19 | `td0_Vpi_heatmap.png` | PNG | **Heatmap VœÄ** (fonction valeur sous politique fixe) | Baseline pr√©diction : montre que politique al√©atoire donne valeurs n√©gatives partout (pas de gradient vers goal). |
| 20 | `td0_pi_followed.png` | PNG | **Politique suivie** (fixe, impos√©e) | Visualiser la politique test√©e (fl√®ches d√©sordonn√©es car al√©atoire). |
| 21 | `td0_convergence.png` | PNG | **Convergence V(s‚ÇÄ)** vs √©pisodes | Stabilisation rapide ~200 ep de V(s‚ÇÄ) autour de -20 (politique sous-optimale). |
| 22 | `td0_pi.csv` | CSV | Politique test√©e (format : state, action) | Reproductibilit√© : permet re-tester exacte m√™me politique. |

**üìù G√©n√©ration** : Script `exp_td0_linear.py` ‚Äî √©value politique fixe avec TD(0).

---

### üìÇ `exp_sarsa_linear/` (3 fichiers)

| # | Fichier | Type | Description | Utilit√© |
|---|---------|------|-------------|----------|
| 23 | `sarsa_V_heatmap.png` | PNG | **V\* (SARSA)** | Comparaison avec TD(0) : valeurs positives pr√®s goal ‚Üí gradient clair (control vs pr√©diction). |
| 24 | `sarsa_pi_grid.png` | PNG | **œÄ\* (SARSA)** | Fl√®ches convergent vers goal mais avec quelques d√©tours (on-policy prudent). |
| 25 | `sarsa_returns.png` | PNG | **Retours par √©pisode** | Croissance progressive jusqu'√† +30-40 (convergence on-policy). |

**üìù G√©n√©ration** : Script `exp_sarsa_linear.py` ‚Äî SARSA control avec approximation lin√©aire.

---

### üìÇ `exp_dqn/` (3 fichiers)

| # | Fichier | Type | Description | Utilit√© |
|---|---------|------|-------------|----------|
| 26 | `dqn_V_heatmap.png` | PNG | **V\* (DQN)** | Valeurs positives optimales (+45-50) ‚Üí meilleur que SARSA (off-policy avantage). |
| 27 | `dqn_pi_grid.png` | PNG | **œÄ\* (DQN)** | Fl√®ches parfaitement align√©es vers goal (politique optimale). |
| 28 | `dqn_returns.png` | PNG | **Retours par √©pisode** | Croissance plus lente mais atteint +45-50 (meilleur final). |

**üìù G√©n√©ration** : Script `exp_dqn.py` ‚Äî DQN avec replay buffer et target network.

---

## S√©ance 4 - DQN Flexible (10 PNG + 1 CSV)

### üìÇ `flex_naive_lin/` (11 fichiers)

| # | Fichier | Type | Dimensions | Description | Utilit√© |
|---|---------|------|-----------|-------------|----------|
| 29 | `V_star_heatmap.png` | PNG | ~800√ó600 | **V\* apprise** (mode naive lin√©aire) | Valide que naive apprend gradient malgr√© absence replay buffer (env 6√ó6 assez simple). |
| 30 | `pi_star_grid.png` | PNG | ~800√ó600 | **œÄ\* (naive)** | Politique coh√©rente ‚Üí convergence OK sur petit espace d'√©tats. |
| 31 | `policy_value.png` | PNG | ~1000√ó600 | **œÄ\* + V\* overlay** | Validation que arg max Q suit gradient V*. |
| 32 | `naive_loss_mean_per_episode.png` | PNG | ~900√ó600 | **TD-loss MSE** par √©pisode (MA50) | D√©croissance de ~30 ‚Üí ~10 apr√®s 800 ep ‚Üí optimiseur converge (r√©sidu = Œµ-greedy stochastique). |
| 33 | `naive_steps_per_episode.png` | PNG | ~900√ó600 | **Steps/√©pisode** (MA50) | D√©croissance de ~40 ‚Üí ~12-15 steps (optimal Manhattan). Proxy de performance. |
| 34 | `naive_epsilon_theta_over_episodes.png` | PNG | ~900√ó600 | **Dual-axis** : Œµ (bleu) + \\|\\|Œ∏\\|\\| (orange) | Valide d√©croissance Œµ et stabilisation poids (~8-10) ‚Üí pas de divergence catastrophique. |
| 35 | `naive_loss_vs_return.png` | PNG | ~800√ó800 | **Scatter plot** loss vs return | Corr√©lation n√©gative : return √©lev√© ‚Üí loss faible. Diagnostique qualit√© approximateur Q. |
| 36 | `visits.png` | PNG | ~800√ó600 | **Heatmap visites** | Distribution uniforme (goals al√©atoires forcent exploration compl√®te). |
| 37 | `dominant_actions.png` | PNG | ~800√ó600 | **Actions dominantes** | Coh√©rence avec œÄ* (empirique ‚âà th√©orique). |
| 38 | `summary_dashboard.png` | PNG | ~1600√ó1200 | **Dashboard 6-en-1** | Vue synth√©tique : reward, convergence, actions, explore/exploit. |
| 39 | `naive_logs.csv` | CSV | - | **Logs par √©pisode** : episode, steps, return, loss, epsilon, theta_norm | Export pour analyses Pandas/Excel. Reproductibilit√© exacte. |

**üìù G√©n√©ration** : Script `exp_flexible.py --mode naive` ‚Äî DQN lin√©aire sans replay/target.

---

## S√©ance 5 - PPO Stable-Baselines3 (8 GIFs + 8 PNG + ~8 CSV)

### üìÇ `gridworld_runs/` (8 GIFs)

| # | Fichier | Type | Dur√©e | FPS | Description | Utilit√© |
|---|---------|------|-------|-----|-------------|----------|
| 40 | `gridworld_ppo_static_50k_live.gif` | GIF | ~5s | 12 | Agent Static 50k steps (en apprentissage) | H√©sitations visibles, quelques d√©tours ‚Üí pas encore optimal. |
| 41 | `gridworld_static_live.gif` | GIF | ~4s | 12 | Agent Static 100k steps (converg√©) | Trajectoires directes vers goal ‚Üí convergence visible. |
| 42 | `gridworld_ppo_static_400k_live.gif` | GIF | ~3s | 12 | Agent Static 400k steps (expert) | Trajectoires parfaites, 12-15 steps optimal. |
| 43 | `gridworld_moving_live.gif` | GIF | ~6s | 12 | Agent Moving 100k steps | Suit goal avec ~2-3 steps de retard. |
| 44 | `gridworld_ppo_moving_400k_live.gif` | GIF | ~5s | 12 | Agent Moving 400k steps | R√©activit√© am√©lior√©e, suit goal rapidement. |
| 45 | `gridworld_ppo_moving_600k_live.gif` | GIF | ~4s | 12 | Agent Moving 600k steps | R√©activit√© quasi-instantan√©e, adaptation optimale. |
| 46 | `gridworld_ppo_moving_finetune_live.gif` | GIF | ~5s | 12 | Agent Fine-tune (Static‚ÜíMoving) | Performances interm√©diaires ~70% du natif. |
| 47 | `test_cartpole.gif` | GIF | ~8s | 12 | CartPole-v1 solved | √âquilibre stable >500 steps (validation pipeline PPO). |

**üìù G√©n√©ration** : Scripts `enjoy.py` de rl-baselines3-zoo avec mod√®les entra√Æn√©s.

**üí° Utilit√© Globale** : Validation qualitative que m√©triques (reward, steps) refl√®tent comportement observ√©.

---

### üìÇ `images/` (8 PNG TensorBoard)

| # | Fichier | Type | Dimensions | Description | Utilit√© |
|---|---------|------|-----------|-------------|----------|
| 48 | `static_50k_400k_ep_rew_mean.png` | PNG | ~1200√ó800 | **Comparaison reward** Static 50k vs 400k | Montre que 50k atteint ~0.90, 400k stabilise √† ~0.95 ‚Üí 50k suffisant pour r√©soudre. |
| 49 | `static_50k_400k_ep_len_mean.png` | PNG | ~1200√ó800 | **Comparaison episode length** Static | D√©croissance de ~25 ‚Üí ~12-15 steps (optimal Manhattan). |
| 50 | `gridworld_static_50k_ep_rew_mean.png` | PNG | ~900√ó600 | **Snapshot reward 50k** | Vue isol√©e 50k steps. |
| 51 | `gridworld_static_400k_ep_rew_mean.png` | PNG | ~900√ó600 | **Snapshot reward 400k** | Vue isol√©e 400k steps. |
| 52 | `moving_400k_600k_1600k_ep_rew_mean.png` | PNG | ~1400√ó800 | **Comparaison reward** Moving 3 runs | 400k ~0.75, 600k ~0.85, 1600k ~0.90 ‚Üí convergence 10√ó plus lente que static. |
| 53 | `moving_400k_600k_1600k_ep_len_mean.png` | PNG | ~1400√ó800 | **Comparaison episode length** Moving | Reste ~15-17 steps (l√©g√®rement > static car r√©activit√© au goal mobile). |
| 54 | `gridworld_moving_400k_ep_rew_mean.png` | PNG | ~900√ó600 | **Snapshot reward Moving 400k** | Vue isol√©e 400k. |
| 55 | `gridworld_moving_600k_ep_rew_mean.png` | PNG | ~900√ó600 | **Snapshot reward Moving 600k** | Vue isol√©e 600k. |

**üìù Source** : TensorBoard logs export√©s en PNG (clics manuels ou `tensorboard --logdir logs`).

**üí° Utilit√©** : Quantifier diff√©rence Static vs Moving (scaling factor ~10√ó), valider que convergence suit trajectoires GIFs.

---

### üìÇ `logs/ppo/.../` (~8 CSV monitor.csv)

**Structure type** :
```csv
# {"t_start": 1234567890.0, "env_id": "GridWorldStatic-v0"}
r,l,t
-5.0,23,0.12
8.5,17,0.25
45.0,12,0.38
```

**Colonnes** : `r` (reward cumul√©), `l` (longueur √©pisode), `t` (timestamp)

**üí° Utilit√©** : Import Pandas pour analyses custom (variance, quantiles, t-tests multi-runs).

---

## R√©capitulatif Global

### üìä Distribution par S√©ance

<div align="center">

| S√©ance | PNG | GIF | CSV | Total | Ratio |
|--------|-----|-----|-----|-------|-------|
| **Seance 2** | 27 | 0 | 1 | 28 | 44% |
| **Seance 4** | 10 | 0 | 1 | 11 | 17% |
| **Seance 5** | 8 | 8 | ~8 | ~24 | 38% |
| **TOTAL** | **45** | **8** | **~10** | **~63** | **100%** |

</div>

### üéØ Utilit√©s Principales par Cat√©gorie

<table>
<tr>
<th width="25%">Cat√©gorie</th>
<th width="15%">Nb Files</th>
<th width="60%">Justification Utilit√©</th>
</tr>
<tr>
<td>üå°Ô∏è <b>Heatmaps V*</b></td>
<td>8</td>
<td>Valider fonction valeur optimale apprise : gradient clair vers goal, d√©tecte sous-optimalit√©, compare algos (TD/SARSA/DQN/Naive).</td>
</tr>
<tr>
<td>üß≠ <b>Politiques œÄ*</b></td>
<td>8</td>
<td>V√©rifier coh√©rence fl√®ches ‚Üí goal, identifier biais (fl√®ches vers obstacles), comparer on-policy vs off-policy.</td>
</tr>
<tr>
<td>üìà <b>Courbes Convergence</b></td>
<td>15</td>
<td>Quantifier vitesse convergence, comparer hyperparam√®tres (Œ≥, grid size), d√©tecter plateaux pr√©matur√©s.</td>
</tr>
<tr>
<td>üìè <b>Episode Length</b></td>
<td>6</td>
<td>Proxy performance : d√©croissance ‚Üí politique plus directe. Valide que agent atteint optimal Manhattan.</td>
</tr>
<tr>
<td>üé¢ <b>Loss/TD-error</b></td>
<td>3</td>
<td>Diagnostic optimiseur : d√©croissance ‚Üí convergence, pics ‚Üí instabilit√©, r√©sidu ‚Üí stochasticit√© env.</td>
</tr>
<tr>
<td>‚öñÔ∏è <b>Explore/Exploit</b></td>
<td>4</td>
<td>Valider schedule Œµ, d√©tecter exploration insuffisante (plateau pr√©matur√©) ou excessive (convergence lente).</td>
</tr>
<tr>
<td>üó∫Ô∏è <b>Visits Heatmaps</b></td>
<td>3</td>
<td>Identifier zones sous-explor√©es (trous coverage) ou sur-visit√©es (biais Œµ-greedy).</td>
</tr>
<tr>
<td>üìä <b>Dashboards</b></td>
<td>3</td>
<td>Vue synth√©tique 6-en-1 : diagnostic rapide multi-m√©trique, gain temps vs lectures multiples.</td>
</tr>
<tr>
<td>üîó <b>Scatter Plots</b></td>
<td>1</td>
<td>Corr√©lation loss vs return : diagnostique qualit√© approximateur Q (surestimation/sous-estimation biais).</td>
</tr>
<tr>
<td>üé¨ <b>GIFs Anim√©s</b></td>
<td>8</td>
<td>Validation qualitative : comportement observ√© ‚âà m√©triques. D√©tecte probl√®mes invisibles dans curves (oscillations, boucles).</td>
</tr>
<tr>
<td>üìÇ <b>CSVs Logs</b></td>
<td>~10</td>
<td>Reproductibilit√©, analyses externes (Pandas, Excel), statistiques avanc√©es (variance inter-runs, t-tests).</td>
</tr>
</table>

### üîç M√©ta-Analyse : Patterns Transversaux

**1. Convergence Multi-Algo** :
- **TD(0)** : Rapide (~200 ep) mais sous-optimal (pr√©diction ‚â† control)
- **SARSA** : Moyenne (~500 ep), on-policy prudent
- **DQN** : Lente (~1000 ep) mais meilleur final (+45-50 vs +30-40)
- **Naive DQN** : Similaire √† DQN sur petit env (6√ó6), divergerait sur grand env
- **PPO** : Tr√®s rapide sur Static (50k), 10√ó plus lent sur Moving (600k)

**2. Sensibilit√© Hyperparam√®tres** :
- **Œ≥** : 0.5-0.7 optimal vitesse, 0.99 optimal qualit√© finale
- **Grid size** : Scaling quadratique (4√ó4: 100 ep, 10√ó10: 1000 ep)
- **Goal mobilit√©** : Static ‚Üí Moving ‚âà 10√ó plus de samples requis

**3. M√©triques Cl√©s** :
- **Reward** : Indicateur principal performance
- **Episode length** : Proxy efficacit√© (optimal ‚âà Manhattan distance)
- **Loss** : Diagnostic convergence optimiseur
- **Œµ empirique** : Validation exploration

**4. Visualisations Critiques** :
- **Heatmaps V\*** : Indispensables pour valider apprentissage
- **Convergence curves** : D√©tectent plateaux/instabilit√©s
- **GIFs** : Seule validation qualitative fiable (m√©triques peuvent mentir)

---

<div align="center">

<br/>

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&color=2ECC71&center=true&vCenter=true&width=600&lines=63+Fichiers+Catalogu√©s+%E2%9C%85;Analyses+Compl√®tes+%E2%9C%85;Justifications+Document√©es+%E2%9C%85" alt="Typing SVG" />

<br/><br/>

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer"/>

</div>
