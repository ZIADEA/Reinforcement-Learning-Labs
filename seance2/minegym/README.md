<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=14,17,20&height=140&section=header&text=Seance%202&fontSize=48&fontColor=fff&animation=fadeIn&fontAlignY=38&desc=GridWorld%20ParamÃ©trable%20%26%20Q-Learning&descAlignY=55&descAlign=50"/>

<br/>

![Q-Learning](https://img.shields.io/badge/Algorithm-Q--Learning-orange?style=for-the-badge&logo=python)
![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)

<br/>

**Environnements GridWorld configurables avec diagnostics Q-Learning complets**

</div>

<br/>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif">

---

## ğŸ“ Note PÃ©dagogique : L'HÃ©ritage du Q-Learning

### ğŸ›ï¸ Le Contexte Historique
Il est impossible de comprendre l'apprentissage par renforcement moderne sans rendre hommage Ã  **Chris Watkins**. En 1989, dans sa thÃ¨se de doctorat Ã  Cambridge intitulÃ©e *"Learning from Delayed Rewards"*, il a introduit l'algorithme **Q-Learning**.

Avant Watkins, le domaine Ã©tait dominÃ© par la Programmation Dynamique (qui nÃ©cessite un modÃ¨le parfait de l'environnement) et les mÃ©thodes Monte Carlo (qui nÃ©cessitent d'attendre la fin d'un Ã©pisode). Watkins a proposÃ© une idÃ©e rÃ©volutionnaire : apprendre la qualitÃ© d'une action (la "Quality" ou **Q-value**) Ã©tape par Ã©tape, sans attendre la fin de l'Ã©pisode et surtout, **sans suivre la politique actuelle**.

### ğŸ’¡ Le Principe Fondamental : Off-Policy vs On-Policy
La distinction majeure que vous devez saisir dans cette sÃ©ance est celle entre **Q-Learning** et **SARSA** (introduit plus tard par Rummery & Niranjan en 1994).

*   **Q-Learning (Off-Policy)** : C'est l'audacieux. Il apprend la valeur de l'action *optimale* ($max Q(s', a')$), mÃªme s'il est en train d'explorer alÃ©atoirement. C'est comme apprendre Ã  jouer aux Ã©checs en regardant un grand maÃ®tre, tout en jouant soi-mÃªme n'importe comment.
    *   *Force* : Converge vers la solution optimale thÃ©orique ($Q^*$) indÃ©pendamment de la faÃ§on dont on explore (tant qu'on explore tout).
*   **SARSA (On-Policy)** : C'est le prudent. Il apprend la valeur de l'action *qu'il va rÃ©ellement prendre* ($Q(s', a')$ selon sa politique actuelle). Il "paie" pour ses erreurs d'exploration.
    *   *Force* : Apprend une politique plus sÃ»re pendant l'entraÃ®nement (Ã©vite les falaises si l'exploration est dangereuse).

### ğŸ”¬ Pourquoi le GridWorld ?
Vous pourriez penser que le GridWorld (monde en grille) est simpliste. DÃ©trompez-vous. C'est la **Drosophile du Reinforcement Learning** (l'organisme modÃ¨le par excellence).
*   **Transparence** : Contrairement Ã  un rÃ©seau de neurones "boÃ®te noire", ici nous pouvons *voir* chaque valeur de Q dans un tableau.
*   **Diagnostic** : Si l'agent ne contourne pas un mur, nous savons exactement pourquoi (la propagation de la rÃ©compense est bloquÃ©e).
*   **UniversalitÃ©** : Les problÃ¨mes de navigation, de labyrinthe et de planification de trajectoire sont les fondements de la robotique mobile.

> **ğŸ“š RÃ©fÃ©rence Incontournable :**
> *Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine learning, 8(3), 279-292.*

---

## ğŸ¯ AperÃ§u

Cette session rend tous les composants du GridWorld configurables (goals, obstacles, cibles mobiles) et se concentre sur l'agent Q-Learning dont le comportement est rÃ©sumÃ© dans les graphiques sous `figures/goal`.

## ğŸš€ DÃ©marrage Rapide

<details open>
<summary><b>âš™ï¸ 1. Activer l'environnement</b></summary>

```powershell
& C:\Users\DJERI\VSCODE\Programmation\python\environnements\rl_venv\Scripts\Activate.ps1
```
</details>

<details>
<summary><b>â–¶ï¸ 2. Lancer les expÃ©riences</b></summary>

```bash
cd seance2
python -m minegym.experiments.liveQL
python -m minegym.experiments.sensitivity_gammaQL
python -m minegym.experiments.sensitivity_grid_sizeQL
```
</details>

<details>
<summary><b>ğŸ“Š 3. Visualiser les rÃ©sultats</b></summary>

Consultez les dashboards, GIFs et heatmaps dans `figures/goal` ou visualisez `live_training.mp4` pour observer la stratÃ©gie de l'agent.
</details>

## ğŸ§ª Suite d'ExpÃ©riences

<table>
<tr>
<th>ğŸ“ Script</th>
<th>ğŸ¯ Objectif</th>
<th>ğŸ“„ Sortie</th>
</tr>
<tr>
<td><code>liveQL</code></td>
<td>Surveiller l'agent avec un flux Matplotlib en direct et logger les dynamiques de rÃ©compenses/Îµ pendant que le GridWorld s'exÃ©cute</td>
<td>
â€¢ <code>live_training.mp4</code><br/>
â€¢ Dashboards de rÃ©compenses<br/>
â€¢ Heatmaps de politique<br/>
â€¢ Visualisations dominance d'actions
</td>
</tr>
<tr>
<td><code>sensitivity_gammaQL</code></td>
<td>Comparer la vitesse de convergence, rÃ©compense finale et aviditÃ© d'exploration pour plusieurs valeurs de Î³</td>
<td>
â€¢ Figures <code>sensitivity_gamma_*</code><br/>
â€¢ Graphiques avec intervalles de confiance<br/>
â€¢ Courbes de croissance<br/>
â€¢ Sous-graphiques proportion greedy
</td>
</tr>
<tr>
<td><code>sensitivity_grid_sizeQL</code></td>
<td>Comparer les mÃªmes statistiques quand la grille augmente de 4Ã—4 Ã  10Ã—10</td>
<td>
â€¢ Figures <code>sensitivity_grid_*</code><br/>
â€¢ Courbes de convergence<br/>
â€¢ Barres de rÃ©compense finale<br/>
â€¢ Portraits d'exploration
</td>
</tr>
</table>

## ğŸ“ Mise Ã  Jour Q-Learning CorrigÃ©e (paramÃ¨tre w)

Au lieu de la cible TD classique, cette sÃ©ance demande un terme de correction `w` qui met Ã  l'Ã©chelle la mise Ã  jour des valeurs d'action :

$$Q(s,a) \leftarrow Q(s,a) + \alpha \cdot w \cdot \left[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right]$$

oÃ¹ `w` ajuste l'agressivitÃ© avec laquelle la valeur tabulaire se dÃ©place vers la cible TD lorsque le monde change (goals mobiles, nouveaux obstacles). Essayez des valeurs entre 0.5 et 1.2 et comparez la vitesse de stabilisation des courbes de rÃ©compense.

## ğŸ–¼ï¸ RÃ©sumÃ© des Sorties Visuelles

<div align="center">

<table>
<tr>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/V_star_heatmap_annotated.png" width="400"/>
<br/><br/>
<b>Heatmap Fonction de Valeur</b>
</td>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/pi_star_grid.png" width="400"/>
<br/><br/>
<b>Grille Politique Optimale</b>
</td>
</tr>
<tr>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/visits.png" width="400"/>
<br/><br/>
<b>Distribution Visites Ã‰tats</b>
</td>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/dominant_actions.png" width="400"/>
<br/><br/>
<b>Actions Dominantes</b>
</td>
</tr>
</table>

### ğŸ“Š Analyse de SensibilitÃ©

<table>
<tr>
<td align="center">
<img src="figures/sensitivity_gamma/sensitivity_gamma_convergence_ci.png" width="600"/>
<br/><br/>
<b>SensibilitÃ© Î³ : Convergence avec Intervalles de Confiance</b>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/sensitivity_grid_size/sensitivity_grid_convergence.png" width="600"/>
<br/><br/>
<b>SensibilitÃ© Taille Grille : Courbes de Convergence</b>
</td>
</tr>
</table>

</div>

---

## âš™ï¸ Modifier l'environnement GridEnv

Le fichier principal pour rÃ©gler la logique du monde est : `minegym/envs/gridworld.py`

---

## ğŸ“Š Galerie ComplÃ¨te des RÃ©sultats Visuels

Cette section prÃ©sente **toutes les visualisations gÃ©nÃ©rÃ©es** par les expÃ©riences, avec analyse de leur utilitÃ©.

### ğŸ¯ LiveQL - EntraÃ®nement en Direct (liveQLgoalsfixed/)

<details open>
<summary><b>ğŸ“¹ Visualisations Principales</b></summary>

<table>
<tr>
<td align="center" width="33%">
<img src="figures/liveQLgoalsfixed/V_star_heatmap_annotated.png" width="100%"/>
<br/><br/>
<b>ğŸŒ¡ï¸ Heatmap V*</b>
<br/>
<sub><i>Fonction de valeur optimale apprise</i></sub>
</td>
<td align="center" width="33%">
<img src="figures/liveQLgoalsfixed/pi_star_grid.png" width="100%"/>
<br/><br/>
<b>ğŸ§­ Politique Optimale</b>
<br/>
<sub><i>FlÃ¨ches indiquant la meilleure action par Ã©tat</i></sub>
</td>
<td align="center" width="33%">
<img src="figures/liveQLgoalsfixed/policy_value.png" width="100%"/>
<br/><br/>
<b>ğŸ¯ Politique + Valeurs</b>
<br/>
<sub><i>Combinaison V* et Ï€*</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse :**
- **V_star_heatmap** : Montre que les valeurs augmentent en se rapprochant du goal (cases chaudes = proches du but)
- **pi_star_grid** : Politique cohÃ©rente - toutes les flÃ¨ches convergent vers le goal
- **policy_value** : Superposition permettant de valider que Ï€* extrait bien l'action qui maximise V*

</details>

<details>
<summary><b>ğŸ“ˆ Analyses Comportementales</b></summary>

<table>
<tr>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/visits.png" width="100%"/>
<br/><br/>
<b>ğŸ—ºï¸ Distribution des Visites</b>
<br/>
<sub><i>Heatmap des Ã©tats explorÃ©s</i></sub>
</td>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/dominant_actions.png" width="100%"/>
<br/><br/>
<b>ğŸ² Actions Dominantes</b>
<br/>
<sub><i>Action la plus frÃ©quente par Ã©tat</i></sub>
</td>
</tr>
<tr>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/live_explore_exploit_empirical.png" width="100%"/>
<br/><br/>
<b>âš–ï¸ Exploration vs Exploitation</b>
<br/>
<sub><i>Proportion d'actions greedy au fil du temps</i></sub>
</td>
<td align="center" width="50%">
<img src="figures/liveQLgoalsfixed/summary_dashboard.png" width="100%"/>
<br/><br/>
<b>ğŸ“Š Dashboard Complet</b>
<br/>
<sub><i>Vue d'ensemble : reward, actions, convergence</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse :**
- **visits.png** : RÃ©vÃ¨le les zones sur-explorÃ©es (utile pour dÃ©tecter des biais) vs zones nÃ©gligÃ©es
- **dominant_actions.png** : Identifie quelle action l'agent privilÃ©gie dans chaque zone (complÃ©ment empirique Ã  Ï€*)
- **live_explore_exploit** : Confirme le dÃ©clin d'Îµ et l'augmentation progressive de la greedy policy (~95% Ã  la fin)
- **summary_dashboard** : Centralise 6 mÃ©triques clÃ©s (convergence reward, longueur Ã©pisodes, distribution actions, etc.) â†’ outil de diagnostic global

</details>

### ğŸ“Š Analyse de SensibilitÃ© Î³ (sensitivity_gamma/)

<details>
<summary><b>ğŸ“‰ Impact du Facteur d'Escompte</b></summary>

<table>
<tr>
<td align="center">
<img src="figures/sensitivity_gamma/sensitivity_gamma_convergence_ci.png" width="100%"/>
<br/><br/>
<b>ğŸ“ˆ Convergence avec Intervalles de Confiance</b>
<br/>
<sub><i>Retour moyen (MA50) pour Î³ âˆˆ [0.0, 0.99] sur 5 seeds</i></sub>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/sensitivity_gamma/sensitivity_gamma_time_to_threshold.png" width="48%"/>
<img src="figures/sensitivity_gamma/sensitivity_gamma_final.png" width="48%"/>
<br/><br/>
<b>â±ï¸ Vitesse de Convergence vs ğŸ† Performance Finale</b>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/sensitivity_gamma/sensitivity_gamma_episode_length.png" width="48%"/>
<img src="figures/sensitivity_gamma/sensitivity_gamma_explore_exploit.png" width="48%"/>
<br/><br/>
<b>ğŸ“ Longueur Ã‰pisodes vs âš–ï¸ Exploration/Exploitation</b>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/sensitivity_gamma/sensitivity_gamma_prop_greedy_subplots.png" width="100%"/>
<br/><br/>
<b>ğŸ¯ Proportion Greedy par Î³ (Subplots)</b>
<br/>
<sub><i>Ã‰volution de la stratÃ©gie gourmande pour chaque valeur de Î³</i></sub>
</td>
</tr>
</table>

**ğŸ“ Analyse Approfondie :**

1. **Convergence avec CI** : 
   - Î³ faibles (0.0-0.4) : convergence rapide mais vers rÃ©compenses sous-optimales (vision courte terme)
   - Î³ moyens (0.5-0.8) : bon compromis vitesse/qualitÃ©
   - Î³ Ã©levÃ©s (0.9-0.99) : meilleure rÃ©compense finale mais convergence plus lente

2. **Time-to-threshold** : Confirme que Î³=0.5-0.7 atteint le seuil -10 le plus rapidement (~400 Ã©pisodes) vs Î³=0.99 (~800 Ã©pisodes)

3. **Performance finale** : Î³=0.99 atteint ~+40 de reward moyen (optimal) vs Î³=0.1 plafonne Ã  ~-5

4. **Episode length** : Î³ Ã©levÃ©s â†’ trajectoires plus longues initialement (exploration profonde) puis stabilisation

5. **Explore/Exploit** : Tous suivent la mÃªme dÃ©croissance d'Îµ, mais Î³ Ã©levÃ©s maintiennent plus d'exploration empirique (biais stochastique)

6. **Prop greedy subplots** : Visualisation individuelle montrant que tous convergent vers >90% greedy sauf Î³=0.0 (reste <80% car politique instable)

**ğŸ’¡ UtilitÃ©** : Guide le choix de Î³ selon l'objectif (vitesse vs qualitÃ© finale)

</details>

### ğŸ“ Analyse de SensibilitÃ© Taille Grille (sensitivity_grid_size/)

<details>
<summary><b>ğŸ“Š Impact de la ComplexitÃ© Spatiale</b></summary>

<table>
<tr>
<td align="center">
<img src="figures/sensitivity_grid_size/sensitivity_grid_convergence.png" width="100%"/>
<br/><br/>
<b>ğŸ“ˆ Convergence selon Taille Grille</b>
<br/>
<sub><i>4Ã—4, 6Ã—6, 8Ã—8, 10Ã—10 comparÃ©s</i></sub>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/sensitivity_grid_size/sensitivity_grid_final.png" width="48%"/>
<img src="figures/sensitivity_grid_size/sensitivity_grid_episode_length.png" width="48%"/>
<br/><br/>
<b>ğŸ† Reward Final vs ğŸ“ Longueur Moyenne Ã‰pisodes</b>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/sensitivity_grid_size/sensitivity_grid_explore_exploit.png" width="48%"/>
<img src="figures/sensitivity_grid_size/sensitivity_grid_prop_greedy_subplots.png" width="48%"/>
<br/><br/>
<b>âš–ï¸ Exploration/Exploitation vs ğŸ¯ Proportion Greedy</b>
</td>
</tr>
</table>

**ğŸ“ Analyse DÃ©taillÃ©e :**

1. **Convergence** :
   - 4Ã—4 : convergence ultra-rapide (<100 Ã©pisodes) - espace d'Ã©tats petit (16 Ã©tats)
   - 10Ã—10 : convergence lente (~1000 Ã©pisodes) - espace d'Ã©tats grand (100 Ã©tats, mais avec obstacles ~70 Ã©tats libres)
   - Scaling non-linÃ©aire : doubler la taille â‰ˆ quadrupler le temps de convergence

2. **Reward final** : Identique pour toutes tailles (~+40-45) car environnements normalisÃ©s (mÃªme reward_goal/step ratio)

3. **Episode length** : CroÃ®t linÃ©airement avec taille (4Ã—4: ~8 steps, 10Ã—10: ~25 steps) â†’ distance Manhattan au goal augmente

4. **Explore/Exploit** : Grilles larges nÃ©cessitent plus d'exploration â†’ proportion greedy monte plus lentement pour 10Ã—10

5. **Prop greedy subplots** : 4Ã—4 atteint 95% greedy dÃ¨s Ã©pisode 200, 10Ã—10 vers Ã©pisode 800

**ğŸ’¡ UtilitÃ©** : Permet d'estimer les ressources computationnelles nÃ©cessaires pour des env plus grands (scaling laws)

</details>

### ğŸ§ª Comparaisons Algorithmiques (exp_td0, exp_sarsa, exp_dqn/)

<details>
<summary><b>ğŸ”¬ TD(0) vs SARSA vs DQN</b></summary>

<table>
<tr>
<th width="33%">TD(0) PrÃ©diction</th>
<th width="33%">SARSA Control</th>
<th width="33%">DQN (Deep)</th>
</tr>
<tr>
<td align="center">
<img src="figures/exp_td0_linear/td0_Vpi_heatmap.png" width="100%"/>
<br/><sub>Fonction V sous politique fixe</sub>
</td>
<td align="center">
<img src="figures/exp_sarsa_linear/sarsa_V_heatmap.png" width="100%"/>
<br/><sub>Fonction V optimale (SARSA)</sub>
</td>
<td align="center">
<img src="figures/exp_dqn/dqn_V_heatmap.png" width="100%"/>
<br/><sub>Fonction V optimale (DQN)</sub>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/exp_td0_linear/td0_pi_followed.png" width="100%"/>
<br/><sub>Politique suivie (fixe)</sub>
</td>
<td align="center">
<img src="figures/exp_sarsa_linear/sarsa_pi_grid.png" width="100%"/>
<br/><sub>Politique apprise (SARSA)</sub>
</td>
<td align="center">
<img src="figures/exp_dqn/dqn_pi_grid.png" width="100%"/>
<br/><sub>Politique apprise (DQN)</sub>
</td>
</tr>
<tr>
<td align="center">
<img src="figures/exp_td0_linear/td0_convergence.png" width="100%"/>
<br/><sub>Convergence V(sâ‚€)</sub>
</td>
<td align="center">
<img src="figures/exp_sarsa_linear/sarsa_returns.png" width="100%"/>
<br/><sub>Retours par Ã©pisode</sub>
</td>
<td align="center">
<img src="figures/exp_dqn/dqn_returns.png" width="100%"/>
<br/><sub>Retours par Ã©pisode</sub>
</td>
</tr>
</table>

**ğŸ“ Comparaison et Justification :**

| Algorithme | Type | V* QualitÃ© | Convergence | UtilitÃ© CSV |
|------------|------|-----------|-------------|-------------|
| **TD(0)** | PrÃ©diction | Faible (politique fixe alÃ©atoire) | Rapide (~200 ep) | `td0_pi.csv` : politique testÃ©e |
| **SARSA** | Control | Bonne (on-policy) | Moyenne (~500 ep) | Aucun CSV gÃ©nÃ©rÃ© |
| **DQN** | Control (Deep) | Excellente (off-policy + rÃ©seau) | Lente (~1000 ep) | Aucun CSV gÃ©nÃ©rÃ© |

**ğŸ” InterprÃ©tations Visuelles :**

1. **Heatmaps V** :
   - TD(0) : Valeurs nÃ©gatives partout (politique sous-optimale qui explore sans but)
   - SARSA/DQN : Valeurs positives prÃ¨s du goal, nÃ©gatives loin â†’ gradient clair vers l'objectif

2. **Politiques Ï€** :
   - TD(0) : FlÃ¨ches dÃ©sordonnÃ©es (politique fixe imposÃ©e)
   - SARSA : FlÃ¨ches convergent vers goal mais avec quelques dÃ©tours (on-policy prudent)
   - DQN : FlÃ¨ches parfaitement alignÃ©es vers goal (off-policy optimal)

3. **Courbes Convergence** :
   - TD(0) : Stabilisation rapide de V(sâ‚€) autour de -20
   - SARSA : Croissance progressive jusqu'Ã  +30-40
   - DQN : Croissance plus lente mais atteint +45-50 (meilleur)

**ğŸ’¡ UtilitÃ©** : DÃ©montre que :
- TD(0) â‰  control (juste Ã©valuation de politique)
- SARSA = bon compromis stabilitÃ©/performance
- DQN = meilleur si on peut se permettre le coÃ»t computationnel

</details>

<br/>

<div align="center">

### ğŸ“¦ RÃ©sumÃ© des Outputs

| Dossier | Images PNG | CSV | UtilitÃ© Principale |
|---------|-----------|-----|-------------------|
| **liveQLgoalsfixed/** | 7 | 0 | Diagnostic complet Q-Learning standard |
| **sensitivity_gamma/** | 6 | 0 | Guide choix hyperparamÃ¨tre Î³ |
| **sensitivity_grid_size/** | 5 | 0 | Scaling laws pour environnements plus grands |
| **exp_td0_linear/** | 3 | 1 | Baseline prÃ©diction (comparaison) |
| **exp_sarsa_linear/** | 3 | 0 | Baseline control on-policy |
| **exp_dqn/** | 3 | 0 | Validation Deep RL sur gridworld |
| **TOTAL** | **27** | **1** | **Analyse exhaustive Q-Learning** |

</div>

<br/>

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=14,17,20&height=100&section=footer"/>

</div>
