<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=255,255,0,0,0&height=140&section=header&text=Seance%203&fontSize=48&fontColor=000&animation=fadeIn&fontAlignY=38&desc=Berkeley%20Pacman%20Project&descAlignY=55&descAlign=50"/>

<br/>

![Pacman](https://img.shields.io/badge/Project-Berkeley_Pacman-yellow?style=for-the-badge&logo=pacman&logoColor=black)
![Algorithm](https://img.shields.io/badge/Algorithm-Approximate_Q--Learning-red?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)

</div>

---

## ðŸŽ“ Note PÃ©dagogique : Le Projet Berkeley Pacman

### ðŸ›ï¸ Les Origines : CS188
Ce projet n'est pas un simple jeu. Il est issu du cÃ©lÃ¨bre cours **CS188 (Introduction to Artificial Intelligence)** de l'UniversitÃ© de Californie Ã  **Berkeley**. ConÃ§u par **John DeNero** et **Dan Klein**, il est devenu la rÃ©fÃ©rence mondiale pour enseigner l'IA.
Pourquoi ? Parce qu'il offre une progression visuelle et intuitive : on commence par des algorithmes de recherche (A*), puis on passe aux MDPs, et enfin au Reinforcement Learning.

### ðŸ§  Le DÃ©fi : L'Explosion Combinatoire
Dans la SÃ©ance 2 (GridWorld), nous utilisions un tableau Q (Q-Table) pour stocker la valeur de chaque case.
Dans Pacman, c'est impossible. Pourquoi ?
*   L'Ã©tat n'est pas juste la position de Pacman (x, y).
*   L'Ã©tat = (Pos Pacman, Pos FantÃ´me 1, Pos FantÃ´me 2, ..., **Ã‰tat de chaque gomme**).
*   S'il y a 30 gommes (food dots), chacune peut Ãªtre mangÃ©e ou non ($2^{30}$ possibilitÃ©s).
*   L'espace d'Ã©tats est astronomique ($> 10^{20}$). Aucun ordinateur ne peut stocker un tableau Q de cette taille.

### ðŸ’¡ La Solution : Approximate Q-Learning
C'est ici que nous introduisons un concept fondamental du RL moderne : l'**Approximation de Fonction**.
Au lieu d'apprendre une valeur pour chaque Ã©tat prÃ©cis (ce qu'on ne reverra jamais deux fois exactement pareil), l'agent apprend Ã  reconnaÃ®tre des **caractÃ©ristiques (features)** :
1.  "Suis-je proche d'un fantÃ´me ?" (Danger)
2.  "Suis-je proche d'une gomme ?" (RÃ©compense)
3.  "Est-ce que je vais dans un cul-de-sac ?"

L'agent apprend des **poids** ($w$) pour ces caractÃ©ristiques.
$$ Q(s, a) = w_1 \cdot f_1(s,a) + w_2 \cdot f_2(s,a) + ... $$
C'est ce qui permet Ã  Pacman de gÃ©nÃ©raliser : s'il apprend que "FantÃ´me proche = Mauvais" dans le coin gauche, il saura que c'est aussi mauvais dans le coin droit.

> **ðŸ“š RÃ©fÃ©rence Incontournable :**
> *DeNero, J., & Klein, D. (2010). Teaching introductory artificial intelligence with Pac-Man. In Proceedings of the Symposium on Educational Advances in Artificial Intelligence (EAAI).*

---

# resultats d une batery de 2000 episodes et 100 test pour chaque algorithme

![alt text](image.png)

avec epsilon=0.05,alpha=0.2,gamma=0.8 dans le medieumclassic world : 
| scores | Value iteration | Qlearning | Qlearning approxiamtif 4features (bias,of-ghosts-1-step-away,eats-food,closest-food) |Qlearning approxiamtif 8features (bias,of-ghosts-1-step-away,eats-food,closest-food,hits-wall,towards-closest-food,ghost-dist,scared-ghost-near) |
|---|---|---|---|---|
| Average Rewards over all training | impossible | -394.07 | 989.07 |1061.27 |
| Average Rewards for last 100 episodes | impossible | -387.97 |958.87 |1089.12 |
| first score after traning | impossible | -369 | 1322 |1344 |
| last score after traning | impossible | -376 | 1332 |1336 |
| Average score  after traning | impossible |  -374.61 | 1224.82 |1229.82 |
<div align="center">
    <img src="pacman_game.gif" alt="Pacman Game Animation" />
</div>

NB : Bien que Pacman soit, en thÃ©orie, modÃ©lisable comme un MDP, le projet Berkeley nâ€™expose pas Pacman via une API MDP (avec getStates, getTransitionStatesAndProbs, getReward, etc.). Par consÃ©quent, lâ€™algorithme dâ€™itÃ©ration de valeur ne peut pas Ãªtre appliquÃ© directement Ã  Pacman et sâ€™utilise sur Gridworld oÃ¹ le MDP est explicite.

